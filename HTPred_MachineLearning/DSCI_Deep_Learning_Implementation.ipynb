{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSCI_Deep_Learning_Implementation.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8a74bd45cb35410088c106d8527e3a41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f28e0613430f4566b4ea94555f868439","IPY_MODEL_c2fce555cd6e4669b934b5994c76a60f","IPY_MODEL_14834a308cf84775bc18ba5e9135289d"],"layout":"IPY_MODEL_372069562fdf4faebb2598e26fda1cd6"}},"f28e0613430f4566b4ea94555f868439":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3bdfb258ad2407f8fa4e68370bc1aa1","placeholder":"​","style":"IPY_MODEL_c42b66b060bb4ee1a4a3290e4a0a1ec1","value":"Summarize dataset: 100%"}},"c2fce555cd6e4669b934b5994c76a60f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f8b47525e574b789745cb52f0d17e6f","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9ba5ba61393467ba39fd2fa2e43b715","value":5}},"14834a308cf84775bc18ba5e9135289d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d65042d71f8a462f944a1644429186bd","placeholder":"​","style":"IPY_MODEL_8c0f087247814ecb95437d7a045a254e","value":" 611/611 [00:08&lt;00:00, 65.96it/s, Completed]"}},"372069562fdf4faebb2598e26fda1cd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3bdfb258ad2407f8fa4e68370bc1aa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c42b66b060bb4ee1a4a3290e4a0a1ec1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f8b47525e574b789745cb52f0d17e6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9ba5ba61393467ba39fd2fa2e43b715":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d65042d71f8a462f944a1644429186bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c0f087247814ecb95437d7a045a254e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53d97f986be043ecbf22984c215e3d7e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c349074aac84a328ed30b01581af00a","IPY_MODEL_ad1cd27518124d24a840e666578f685d","IPY_MODEL_a24c2c3ea30a4d739f33f86cd3d5a3c4"],"layout":"IPY_MODEL_f0f5ff94963949fa99cb22266f258f2f"}},"1c349074aac84a328ed30b01581af00a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66bd81918c0f4bcf8189149cae4fb623","placeholder":"​","style":"IPY_MODEL_1fe9f094c2f248c4ae6263c99f27b578","value":"Generate report structure: 100%"}},"ad1cd27518124d24a840e666578f685d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c50c303f6ef4b009adfe31d4e0522dc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_008ecaa597174100be73bc08234e4a78","value":1}},"a24c2c3ea30a4d739f33f86cd3d5a3c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b2d8d79a6a483d89753780f3632dbf","placeholder":"​","style":"IPY_MODEL_7be79ee1aedd4fb395f9b0e9a3b97057","value":" 1/1 [05:05&lt;00:00, 305.21s/it]"}},"f0f5ff94963949fa99cb22266f258f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66bd81918c0f4bcf8189149cae4fb623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fe9f094c2f248c4ae6263c99f27b578":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c50c303f6ef4b009adfe31d4e0522dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"008ecaa597174100be73bc08234e4a78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98b2d8d79a6a483d89753780f3632dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7be79ee1aedd4fb395f9b0e9a3b97057":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"402890a49dbd443ca76f4c3890d204fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ef42d148edb4176b86aa30efbede769","IPY_MODEL_6c8404bb8f4c438eba77a260c3a0f119","IPY_MODEL_dc87be1733b143f4b28b58c0a5f00f9f"],"layout":"IPY_MODEL_c93d66df2d834329bbeee25455a3aedd"}},"2ef42d148edb4176b86aa30efbede769":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e58b4ba46984cdf89dcb201a22823af","placeholder":"​","style":"IPY_MODEL_1779f8334b8f4576953df648f3e6d4cc","value":"Render HTML: 100%"}},"6c8404bb8f4c438eba77a260c3a0f119":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e33a3d1348914cd0b2b17ee5f67c89de","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62ab2ca86ffd4e17b3d1e95dfe4b20d4","value":1}},"dc87be1733b143f4b28b58c0a5f00f9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f4a3df01df04660a9714e209b3dbc58","placeholder":"​","style":"IPY_MODEL_942e50453ad84609844f211515f0c8cf","value":" 1/1 [00:19&lt;00:00, 19.24s/it]"}},"c93d66df2d834329bbeee25455a3aedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e58b4ba46984cdf89dcb201a22823af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1779f8334b8f4576953df648f3e6d4cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e33a3d1348914cd0b2b17ee5f67c89de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ab2ca86ffd4e17b3d1e95dfe4b20d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f4a3df01df04660a9714e209b3dbc58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"942e50453ad84609844f211515f0c8cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Importing Libraries"],"metadata":{"id":"kpJvxYdpAlsT"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","import numpy as np\n","import pandas as pd\n","import pandas_profiling\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import minmax_scale\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import svm\n","from sklearn.tree import DecisionTreeClassifier \n","from sklearn import metrics\n","from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n","from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n","from sklearn.metrics import auc, plot_precision_recall_curve\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential,model_from_json\n","from keras.layers import Dense\n","from keras.optimizers import rmsprop_v2\n","import pylab as plt\n","from sklearn.utils.class_weight import compute_class_weight"],"metadata":{"id":"tORRP4GwAmrV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importing Dataset"],"metadata":{"id":"FHV4JtKLAdrr"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset_HTPred_classification.csv\",na_values=\"?\")\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"f03MniPUAcng","executionInfo":{"status":"ok","timestamp":1656437064732,"user_tz":-330,"elapsed":4489,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"182656ab-c323-495a-f0c7-3521155790cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["               Circuit Name  No. of primary input gates  \\\n","0  AES_128_TjFree_bench.txt                         257   \n","1                 b01.bench                           2   \n","2                 b02.bench                           1   \n","3                 b03.bench                           4   \n","4                 b04.bench                          11   \n","\n","   No. of primary output gates  No. of wires  No. of non-primary input gates  \\\n","0                          128        970168                          661048   \n","1                            2           120                              85   \n","2                            1            67                              48   \n","3                            4           424                             284   \n","4                            8          1927                            1333   \n","\n","   No. of non-primary output gates  No. of 3 or more input gates  \\\n","0                           316608                         55232   \n","1                               45                             8   \n","2                               26                             4   \n","3                              152                            20   \n","4                              718                            53   \n","\n","        sum CC0       sum CC1        sum CO  ...  Obth L =  60  Obth L =  65  \\\n","0  1.879040e+07  2.583875e+07  1.062307e+09  ...  2.744000e+03  2.401000e+03   \n","1  4.337000e+03  2.640000e+03  1.723580e+05  ...  2.570000e+03  2.248750e+03   \n","2  7.799000e+03  4.656000e+03  1.764280e+05  ...  3.725600e+03  3.259900e+03   \n","3  5.520400e+04  3.799100e+04  5.664984e+06  ...  2.083440e+04  1.823010e+04   \n","4  7.510000e+13  4.150000e+13  3.720000e+15  ...  3.900000e+12  3.410000e+12   \n","\n","   Obth L =  70  Obth L =  75  Obth L =  80  Obth L =  85  Obth L =  90  \\\n","0  2.058000e+03  1.715000e+03  1.372000e+03  1.029000e+03  6.860000e+02   \n","1  1.927500e+03  1.606250e+03  1.285000e+03  9.637500e+02  6.425000e+02   \n","2  2.794200e+03  2.328500e+03  1.862800e+03  1.397100e+03  9.314000e+02   \n","3  1.562580e+04  1.302150e+04  1.041720e+04  7.812900e+03  5.208600e+03   \n","4  2.920000e+12  2.440000e+12  1.950000e+12  1.460000e+12  9.750000e+11   \n","\n","   Obth L =  95  Obth L =  100       Class  \n","0  3.430000e+02            0.0  Non-Trojan  \n","1  3.212500e+02            0.0  Non-Trojan  \n","2  4.657000e+02            0.0  Non-Trojan  \n","3  2.604300e+03            0.0  Non-Trojan  \n","4  4.870000e+11            0.0  Non-Trojan  \n","\n","[5 rows x 605 columns]"],"text/html":["\n","  <div id=\"df-a9cf2a38-50f2-4616-ae6f-bba50fb024b2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Circuit Name</th>\n","      <th>No. of primary input gates</th>\n","      <th>No. of primary output gates</th>\n","      <th>No. of wires</th>\n","      <th>No. of non-primary input gates</th>\n","      <th>No. of non-primary output gates</th>\n","      <th>No. of 3 or more input gates</th>\n","      <th>sum CC0</th>\n","      <th>sum CC1</th>\n","      <th>sum CO</th>\n","      <th>...</th>\n","      <th>Obth L =  60</th>\n","      <th>Obth L =  65</th>\n","      <th>Obth L =  70</th>\n","      <th>Obth L =  75</th>\n","      <th>Obth L =  80</th>\n","      <th>Obth L =  85</th>\n","      <th>Obth L =  90</th>\n","      <th>Obth L =  95</th>\n","      <th>Obth L =  100</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AES_128_TjFree_bench.txt</td>\n","      <td>257</td>\n","      <td>128</td>\n","      <td>970168</td>\n","      <td>661048</td>\n","      <td>316608</td>\n","      <td>55232</td>\n","      <td>1.879040e+07</td>\n","      <td>2.583875e+07</td>\n","      <td>1.062307e+09</td>\n","      <td>...</td>\n","      <td>2.744000e+03</td>\n","      <td>2.401000e+03</td>\n","      <td>2.058000e+03</td>\n","      <td>1.715000e+03</td>\n","      <td>1.372000e+03</td>\n","      <td>1.029000e+03</td>\n","      <td>6.860000e+02</td>\n","      <td>3.430000e+02</td>\n","      <td>0.0</td>\n","      <td>Non-Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>b01.bench</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>120</td>\n","      <td>85</td>\n","      <td>45</td>\n","      <td>8</td>\n","      <td>4.337000e+03</td>\n","      <td>2.640000e+03</td>\n","      <td>1.723580e+05</td>\n","      <td>...</td>\n","      <td>2.570000e+03</td>\n","      <td>2.248750e+03</td>\n","      <td>1.927500e+03</td>\n","      <td>1.606250e+03</td>\n","      <td>1.285000e+03</td>\n","      <td>9.637500e+02</td>\n","      <td>6.425000e+02</td>\n","      <td>3.212500e+02</td>\n","      <td>0.0</td>\n","      <td>Non-Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>b02.bench</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>67</td>\n","      <td>48</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>7.799000e+03</td>\n","      <td>4.656000e+03</td>\n","      <td>1.764280e+05</td>\n","      <td>...</td>\n","      <td>3.725600e+03</td>\n","      <td>3.259900e+03</td>\n","      <td>2.794200e+03</td>\n","      <td>2.328500e+03</td>\n","      <td>1.862800e+03</td>\n","      <td>1.397100e+03</td>\n","      <td>9.314000e+02</td>\n","      <td>4.657000e+02</td>\n","      <td>0.0</td>\n","      <td>Non-Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>b03.bench</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>424</td>\n","      <td>284</td>\n","      <td>152</td>\n","      <td>20</td>\n","      <td>5.520400e+04</td>\n","      <td>3.799100e+04</td>\n","      <td>5.664984e+06</td>\n","      <td>...</td>\n","      <td>2.083440e+04</td>\n","      <td>1.823010e+04</td>\n","      <td>1.562580e+04</td>\n","      <td>1.302150e+04</td>\n","      <td>1.041720e+04</td>\n","      <td>7.812900e+03</td>\n","      <td>5.208600e+03</td>\n","      <td>2.604300e+03</td>\n","      <td>0.0</td>\n","      <td>Non-Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>b04.bench</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>1927</td>\n","      <td>1333</td>\n","      <td>718</td>\n","      <td>53</td>\n","      <td>7.510000e+13</td>\n","      <td>4.150000e+13</td>\n","      <td>3.720000e+15</td>\n","      <td>...</td>\n","      <td>3.900000e+12</td>\n","      <td>3.410000e+12</td>\n","      <td>2.920000e+12</td>\n","      <td>2.440000e+12</td>\n","      <td>1.950000e+12</td>\n","      <td>1.460000e+12</td>\n","      <td>9.750000e+11</td>\n","      <td>4.870000e+11</td>\n","      <td>0.0</td>\n","      <td>Non-Trojan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 605 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9cf2a38-50f2-4616-ae6f-bba50fb024b2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a9cf2a38-50f2-4616-ae6f-bba50fb024b2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a9cf2a38-50f2-4616-ae6f-bba50fb024b2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["Exploratory Data Analysis"],"metadata":{"id":"PF6Owg-AC-Pg"}},{"cell_type":"code","source":["pandas_profiling.ProfileReport(data, minimal=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":918,"referenced_widgets":["8a74bd45cb35410088c106d8527e3a41","f28e0613430f4566b4ea94555f868439","c2fce555cd6e4669b934b5994c76a60f","14834a308cf84775bc18ba5e9135289d","372069562fdf4faebb2598e26fda1cd6","f3bdfb258ad2407f8fa4e68370bc1aa1","c42b66b060bb4ee1a4a3290e4a0a1ec1","5f8b47525e574b789745cb52f0d17e6f","d9ba5ba61393467ba39fd2fa2e43b715","d65042d71f8a462f944a1644429186bd","8c0f087247814ecb95437d7a045a254e","53d97f986be043ecbf22984c215e3d7e","1c349074aac84a328ed30b01581af00a","ad1cd27518124d24a840e666578f685d","a24c2c3ea30a4d739f33f86cd3d5a3c4","f0f5ff94963949fa99cb22266f258f2f","66bd81918c0f4bcf8189149cae4fb623","1fe9f094c2f248c4ae6263c99f27b578","8c50c303f6ef4b009adfe31d4e0522dc","008ecaa597174100be73bc08234e4a78","98b2d8d79a6a483d89753780f3632dbf","7be79ee1aedd4fb395f9b0e9a3b97057","402890a49dbd443ca76f4c3890d204fb","2ef42d148edb4176b86aa30efbede769","6c8404bb8f4c438eba77a260c3a0f119","dc87be1733b143f4b28b58c0a5f00f9f","c93d66df2d834329bbeee25455a3aedd","2e58b4ba46984cdf89dcb201a22823af","1779f8334b8f4576953df648f3e6d4cc","e33a3d1348914cd0b2b17ee5f67c89de","62ab2ca86ffd4e17b3d1e95dfe4b20d4","2f4a3df01df04660a9714e209b3dbc58","942e50453ad84609844f211515f0c8cf"],"output_embedded_package_id":"1MulCPj_7umJW7YiDyg65nr7tnSfUfrsD"},"id":"_3AaGTGiDAer","executionInfo":{"status":"ok","timestamp":1649495627147,"user_tz":-330,"elapsed":345445,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"2cf004bf-b0b1-4617-8ce2-518722d4e6c4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Data Preprocessing"],"metadata":{"id":"50sKmA8HFMh6"}},{"cell_type":"code","source":["#Uisng Label Encoder\n","data_string = data.dtypes==object\n","data_cols = data.columns[data_string].tolist()\n","le = LabelEncoder()\n","data[data_cols] = data[data_cols].apply(lambda col: le.fit_transform(col))\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"kOZkMgMJMVGs","executionInfo":{"status":"ok","timestamp":1656437069881,"user_tz":-330,"elapsed":397,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"f085e7fd-eae5-411d-cf55-1510236e2ae6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Circuit Name  No. of primary input gates  No. of primary output gates  \\\n","0             0                         257                          128   \n","1             7                           2                            2   \n","2             8                           1                            1   \n","3             9                           4                            4   \n","4            10                          11                            8   \n","\n","   No. of wires  No. of non-primary input gates  \\\n","0        970168                          661048   \n","1           120                              85   \n","2            67                              48   \n","3           424                             284   \n","4          1927                            1333   \n","\n","   No. of non-primary output gates  No. of 3 or more input gates  \\\n","0                           316608                         55232   \n","1                               45                             8   \n","2                               26                             4   \n","3                              152                            20   \n","4                              718                            53   \n","\n","        sum CC0       sum CC1        sum CO  ...  Obth L =  60  Obth L =  65  \\\n","0  1.879040e+07  2.583875e+07  1.062307e+09  ...  2.744000e+03  2.401000e+03   \n","1  4.337000e+03  2.640000e+03  1.723580e+05  ...  2.570000e+03  2.248750e+03   \n","2  7.799000e+03  4.656000e+03  1.764280e+05  ...  3.725600e+03  3.259900e+03   \n","3  5.520400e+04  3.799100e+04  5.664984e+06  ...  2.083440e+04  1.823010e+04   \n","4  7.510000e+13  4.150000e+13  3.720000e+15  ...  3.900000e+12  3.410000e+12   \n","\n","   Obth L =  70  Obth L =  75  Obth L =  80  Obth L =  85  Obth L =  90  \\\n","0  2.058000e+03  1.715000e+03  1.372000e+03  1.029000e+03  6.860000e+02   \n","1  1.927500e+03  1.606250e+03  1.285000e+03  9.637500e+02  6.425000e+02   \n","2  2.794200e+03  2.328500e+03  1.862800e+03  1.397100e+03  9.314000e+02   \n","3  1.562580e+04  1.302150e+04  1.041720e+04  7.812900e+03  5.208600e+03   \n","4  2.920000e+12  2.440000e+12  1.950000e+12  1.460000e+12  9.750000e+11   \n","\n","   Obth L =  95  Obth L =  100  Class  \n","0  3.430000e+02            0.0      0  \n","1  3.212500e+02            0.0      0  \n","2  4.657000e+02            0.0      0  \n","3  2.604300e+03            0.0      0  \n","4  4.870000e+11            0.0      0  \n","\n","[5 rows x 605 columns]"],"text/html":["\n","  <div id=\"df-0969b48b-401e-4f56-bd5f-8947f65ccfae\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Circuit Name</th>\n","      <th>No. of primary input gates</th>\n","      <th>No. of primary output gates</th>\n","      <th>No. of wires</th>\n","      <th>No. of non-primary input gates</th>\n","      <th>No. of non-primary output gates</th>\n","      <th>No. of 3 or more input gates</th>\n","      <th>sum CC0</th>\n","      <th>sum CC1</th>\n","      <th>sum CO</th>\n","      <th>...</th>\n","      <th>Obth L =  60</th>\n","      <th>Obth L =  65</th>\n","      <th>Obth L =  70</th>\n","      <th>Obth L =  75</th>\n","      <th>Obth L =  80</th>\n","      <th>Obth L =  85</th>\n","      <th>Obth L =  90</th>\n","      <th>Obth L =  95</th>\n","      <th>Obth L =  100</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>257</td>\n","      <td>128</td>\n","      <td>970168</td>\n","      <td>661048</td>\n","      <td>316608</td>\n","      <td>55232</td>\n","      <td>1.879040e+07</td>\n","      <td>2.583875e+07</td>\n","      <td>1.062307e+09</td>\n","      <td>...</td>\n","      <td>2.744000e+03</td>\n","      <td>2.401000e+03</td>\n","      <td>2.058000e+03</td>\n","      <td>1.715000e+03</td>\n","      <td>1.372000e+03</td>\n","      <td>1.029000e+03</td>\n","      <td>6.860000e+02</td>\n","      <td>3.430000e+02</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>120</td>\n","      <td>85</td>\n","      <td>45</td>\n","      <td>8</td>\n","      <td>4.337000e+03</td>\n","      <td>2.640000e+03</td>\n","      <td>1.723580e+05</td>\n","      <td>...</td>\n","      <td>2.570000e+03</td>\n","      <td>2.248750e+03</td>\n","      <td>1.927500e+03</td>\n","      <td>1.606250e+03</td>\n","      <td>1.285000e+03</td>\n","      <td>9.637500e+02</td>\n","      <td>6.425000e+02</td>\n","      <td>3.212500e+02</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>67</td>\n","      <td>48</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>7.799000e+03</td>\n","      <td>4.656000e+03</td>\n","      <td>1.764280e+05</td>\n","      <td>...</td>\n","      <td>3.725600e+03</td>\n","      <td>3.259900e+03</td>\n","      <td>2.794200e+03</td>\n","      <td>2.328500e+03</td>\n","      <td>1.862800e+03</td>\n","      <td>1.397100e+03</td>\n","      <td>9.314000e+02</td>\n","      <td>4.657000e+02</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>424</td>\n","      <td>284</td>\n","      <td>152</td>\n","      <td>20</td>\n","      <td>5.520400e+04</td>\n","      <td>3.799100e+04</td>\n","      <td>5.664984e+06</td>\n","      <td>...</td>\n","      <td>2.083440e+04</td>\n","      <td>1.823010e+04</td>\n","      <td>1.562580e+04</td>\n","      <td>1.302150e+04</td>\n","      <td>1.041720e+04</td>\n","      <td>7.812900e+03</td>\n","      <td>5.208600e+03</td>\n","      <td>2.604300e+03</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>1927</td>\n","      <td>1333</td>\n","      <td>718</td>\n","      <td>53</td>\n","      <td>7.510000e+13</td>\n","      <td>4.150000e+13</td>\n","      <td>3.720000e+15</td>\n","      <td>...</td>\n","      <td>3.900000e+12</td>\n","      <td>3.410000e+12</td>\n","      <td>2.920000e+12</td>\n","      <td>2.440000e+12</td>\n","      <td>1.950000e+12</td>\n","      <td>1.460000e+12</td>\n","      <td>9.750000e+11</td>\n","      <td>4.870000e+11</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 605 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0969b48b-401e-4f56-bd5f-8947f65ccfae')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0969b48b-401e-4f56-bd5f-8947f65ccfae button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0969b48b-401e-4f56-bd5f-8947f65ccfae');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["X = data.iloc[:, 1:-1].values\n","y = data.iloc[:, -1].values\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","class_weights = compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(y_train),\n","                                        y = y_train                                                    \n","                                    )\n","\n","class_weights = dict(zip(np.unique(y_train), class_weights))\n","class_weights"],"metadata":{"id":"tOx-y2SuFQPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656437075923,"user_tz":-330,"elapsed":584,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"a40e973d-dbaa-4704-cc80-79090bf8371e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 3.23728813559322, 1: 0.5913312693498453}"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["Creating Deep Neural Networks"],"metadata":{"id":"5WOwMnQZFh_S"}},{"cell_type":"code","source":["# Deep Learning Model - All Feature Config 1\n","### Batch Size : 32\n","### Epochs: 24\n","### L1: 20    \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"id":"Fcb9eTTkFnSz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649490471044,"user_tz":-330,"elapsed":4453,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"4731e903-9b60-4442-bb80-c6ee8900007c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","12/12 [==============================] - 2s 7ms/step - loss: 0.6123 - accuracy: 0.7408\n","Epoch 2/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4735 - accuracy: 0.8010\n","Epoch 3/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.3809 - accuracy: 0.8063\n","Epoch 4/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.3006 - accuracy: 0.8351\n","Epoch 5/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2668 - accuracy: 0.9764\n","Epoch 6/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2368 - accuracy: 0.9791\n","Epoch 7/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9817\n","Epoch 8/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 0.9791\n","Epoch 9/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.1914 - accuracy: 0.9843\n","Epoch 10/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9843\n","Epoch 11/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.1760 - accuracy: 0.9843\n","Epoch 12/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 0.9869\n","Epoch 13/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.1544 - accuracy: 0.9843\n","Epoch 14/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.1446 - accuracy: 0.9869\n","Epoch 15/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9869\n","Epoch 16/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.1425 - accuracy: 0.9843\n","Epoch 17/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9843\n","Epoch 18/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.9869\n","Epoch 19/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9791\n","Epoch 20/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9895\n","Epoch 21/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9921\n","Epoch 22/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1023 - accuracy: 0.9869\n","Epoch 23/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.0946 - accuracy: 0.9921\n","Epoch 24/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.0965 - accuracy: 0.9843\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9895833333333334\n","TP Rate:  82\n","FP Rate:  1\n","Precision:  0.9879518072289156\n","Recall:  1.0\n","F1 Score:  0.993939393939394\n","MCC:  0.9578015561682279\n","ROC Area:  0.9642857142857143\n","PRC Area:  0.9939759036144578\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - All Feature Config 2\n","### Batch Size : 32\n","### Epochs: 200\n","### L1: 20    \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEI7TEfeR-o_","executionInfo":{"status":"ok","timestamp":1649490490861,"user_tz":-330,"elapsed":11379,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"8b961ef4-09f0-4dfc-df24-73adcb8c7ee2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","12/12 [==============================] - 1s 3ms/step - loss: 0.7474 - accuracy: 0.7408\n","Epoch 2/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5706 - accuracy: 0.7827\n","Epoch 3/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.7853\n","Epoch 4/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.7984\n","Epoch 5/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8037\n","Epoch 6/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8063\n","Epoch 7/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8063\n","Epoch 8/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2454 - accuracy: 0.8455\n","Epoch 9/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9817\n","Epoch 10/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9843\n","Epoch 11/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9843\n","Epoch 12/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9843\n","Epoch 13/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9555\n","Epoch 14/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9843\n","Epoch 15/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.1609 - accuracy: 0.9843\n","Epoch 16/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9869\n","Epoch 17/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9843\n","Epoch 18/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.9843\n","Epoch 19/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.9843\n","Epoch 20/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1362 - accuracy: 0.9843\n","Epoch 21/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9869\n","Epoch 22/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9869\n","Epoch 23/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9869\n","Epoch 24/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1249 - accuracy: 0.9869\n","Epoch 25/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9869\n","Epoch 26/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9791\n","Epoch 27/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9843\n","Epoch 28/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1158 - accuracy: 0.9817\n","Epoch 29/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9869\n","Epoch 30/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9817\n","Epoch 31/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9895\n","Epoch 32/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.9791\n","Epoch 33/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9921\n","Epoch 34/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9921\n","Epoch 35/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9895\n","Epoch 36/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9764\n","Epoch 37/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9817\n","Epoch 38/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.9895\n","Epoch 39/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9921\n","Epoch 40/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9921\n","Epoch 41/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9712\n","Epoch 42/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9895\n","Epoch 43/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9869\n","Epoch 44/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0929 - accuracy: 0.9869\n","Epoch 45/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9921\n","Epoch 46/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9869\n","Epoch 47/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9895\n","Epoch 48/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9843\n","Epoch 49/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9921\n","Epoch 50/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9921\n","Epoch 51/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9895\n","Epoch 52/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9895\n","Epoch 53/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9843\n","Epoch 54/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9869\n","Epoch 55/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9869\n","Epoch 56/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9895\n","Epoch 57/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9921\n","Epoch 58/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.9895\n","Epoch 59/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9869\n","Epoch 60/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9869\n","Epoch 61/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9921\n","Epoch 62/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9712\n","Epoch 63/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9895\n","Epoch 64/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9869\n","Epoch 65/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9921\n","Epoch 66/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9921\n","Epoch 67/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9791\n","Epoch 68/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9921\n","Epoch 69/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9817\n","Epoch 70/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9895\n","Epoch 71/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9791\n","Epoch 72/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9921\n","Epoch 73/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9921\n","Epoch 74/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9895\n","Epoch 75/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9921\n","Epoch 76/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9738\n","Epoch 77/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9921\n","Epoch 78/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9895\n","Epoch 79/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9895\n","Epoch 80/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9895\n","Epoch 81/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9921\n","Epoch 82/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9843\n","Epoch 83/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9895\n","Epoch 84/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9843\n","Epoch 85/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9921\n","Epoch 86/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9921\n","Epoch 87/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9869\n","Epoch 88/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9817\n","Epoch 89/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9738\n","Epoch 90/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9921\n","Epoch 91/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9869\n","Epoch 92/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9895\n","Epoch 93/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9921\n","Epoch 94/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9817\n","Epoch 95/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9895\n","Epoch 96/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9843\n","Epoch 97/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9895\n","Epoch 98/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9921\n","Epoch 99/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9843\n","Epoch 100/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9869\n","Epoch 101/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9948\n","Epoch 102/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9921\n","Epoch 103/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9817\n","Epoch 104/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9895\n","Epoch 105/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9895\n","Epoch 106/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9895\n","Epoch 107/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9921\n","Epoch 108/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9843\n","Epoch 109/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9843\n","Epoch 110/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9869\n","Epoch 111/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9921\n","Epoch 112/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9764\n","Epoch 113/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9895\n","Epoch 114/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9791\n","Epoch 115/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9843\n","Epoch 116/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9869\n","Epoch 117/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9921\n","Epoch 118/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9817\n","Epoch 119/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9921\n","Epoch 120/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9738\n","Epoch 121/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9921\n","Epoch 122/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9791\n","Epoch 123/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9895\n","Epoch 124/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9948\n","Epoch 125/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.9869\n","Epoch 126/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9921\n","Epoch 127/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9712\n","Epoch 128/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9921\n","Epoch 129/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9869\n","Epoch 130/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9869\n","Epoch 131/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9869\n","Epoch 132/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9921\n","Epoch 133/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9817\n","Epoch 134/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9895\n","Epoch 135/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9686\n","Epoch 136/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9895\n","Epoch 137/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9817\n","Epoch 138/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9921\n","Epoch 139/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9869\n","Epoch 140/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9817\n","Epoch 141/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9921\n","Epoch 142/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9921\n","Epoch 143/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9921\n","Epoch 144/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9895\n","Epoch 145/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9895\n","Epoch 146/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0539 - accuracy: 0.9921\n","Epoch 147/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9895\n","Epoch 148/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9869\n","Epoch 149/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9869\n","Epoch 150/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9895\n","Epoch 151/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9791\n","Epoch 152/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9921\n","Epoch 153/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9895\n","Epoch 154/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9869\n","Epoch 155/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9869\n","Epoch 156/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9817\n","Epoch 157/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9843\n","Epoch 158/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9948\n","Epoch 159/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9921\n","Epoch 160/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9764\n","Epoch 161/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9921\n","Epoch 162/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9712\n","Epoch 163/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9921\n","Epoch 164/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9843\n","Epoch 165/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9948\n","Epoch 166/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9843\n","Epoch 167/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9948\n","Epoch 168/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9921\n","Epoch 169/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9869\n","Epoch 170/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9869\n","Epoch 171/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9895\n","Epoch 172/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9921\n","Epoch 173/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9712\n","Epoch 174/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9921\n","Epoch 175/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9843\n","Epoch 176/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9869\n","Epoch 177/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9712\n","Epoch 178/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9869\n","Epoch 179/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0468 - accuracy: 0.9869\n","Epoch 180/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9843\n","Epoch 181/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9791\n","Epoch 182/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9764\n","Epoch 183/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9921\n","Epoch 184/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9869\n","Epoch 185/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9895\n","Epoch 186/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9895\n","Epoch 187/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9817\n","Epoch 188/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9869\n","Epoch 189/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9869\n","Epoch 190/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9843\n","Epoch 191/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.9843\n","Epoch 192/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9895\n","Epoch 193/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0461 - accuracy: 0.9791\n","Epoch 194/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9921\n","Epoch 195/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.9843\n","Epoch 196/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9921\n","Epoch 197/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9843\n","Epoch 198/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9895\n","Epoch 199/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9738\n","Epoch 200/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 0.9921\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7dfc6a24d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9895833333333334\n","TP Rate:  82\n","FP Rate:  1\n","Precision:  0.9879518072289156\n","Recall:  1.0\n","F1 Score:  0.993939393939394\n","MCC:  0.9578015561682279\n","ROC Area:  0.9642857142857143\n","PRC Area:  0.9939759036144578\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - All Feature Config 3\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 2\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fcpRV83TAly","executionInfo":{"status":"ok","timestamp":1649490604213,"user_tz":-330,"elapsed":3951,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"252b4e38-1ff9-4e88-a1b8-2138361ce29a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.8141\n","Epoch 2/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6771 - accuracy: 0.7723\n","Epoch 3/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.5550 - accuracy: 0.8010\n","Epoch 4/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.8403\n","Epoch 5/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.9372\n","Epoch 6/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8901\n","Epoch 7/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 0.9607\n","Epoch 8/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.9843\n","Epoch 9/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.9503\n","Epoch 10/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.9817\n","Epoch 11/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.9895\n","Epoch 12/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.9660\n","Epoch 13/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.9738\n","Epoch 14/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.9843\n","Epoch 15/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.9712\n","Epoch 16/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.9869\n","Epoch 17/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9869\n","Epoch 18/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9791\n","Epoch 19/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9843\n","Epoch 20/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1922 - accuracy: 0.9895\n","Epoch 21/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.1842 - accuracy: 0.9869\n","Epoch 22/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9895\n","Epoch 23/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1699 - accuracy: 0.9869\n","Epoch 24/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9869\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9895833333333334\n","TP Rate:  82\n","FP Rate:  1\n","Precision:  0.9879518072289156\n","Recall:  1.0\n","F1 Score:  0.993939393939394\n","MCC:  0.9578015561682279\n","ROC Area:  0.9642857142857143\n","PRC Area:  0.9939759036144578\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - All Feature Config 4\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dp-LUYcqTb2Y","executionInfo":{"status":"ok","timestamp":1649490693499,"user_tz":-330,"elapsed":5145,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"1f290d70-5a30-4fe5-94d1-09f96af759e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 6ms/step - loss: 0.6619 - accuracy: 0.8298\n","Epoch 2/24\n","32/32 [==============================] - 0s 6ms/step - loss: 0.5772 - accuracy: 0.8141\n","Epoch 3/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5481 - accuracy: 0.7932\n","Epoch 4/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7906\n","Epoch 5/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5127 - accuracy: 0.7932\n","Epoch 6/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4947 - accuracy: 0.8063\n","Epoch 7/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.8063\n","Epoch 8/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.8089\n","Epoch 9/24\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.8089\n","Epoch 10/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4826 - accuracy: 0.8063\n","Epoch 11/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.8089\n","Epoch 12/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.8115\n","Epoch 13/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4501 - accuracy: 0.8115\n","Epoch 14/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.8089\n","Epoch 15/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.8089\n","Epoch 16/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4470 - accuracy: 0.8089\n","Epoch 17/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4430 - accuracy: 0.8115\n","Epoch 18/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4381 - accuracy: 0.8089\n","Epoch 19/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4517 - accuracy: 0.8089\n","Epoch 20/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7984\n","Epoch 21/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.8089\n","Epoch 22/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.8063\n","Epoch 23/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.8089\n","Epoch 24/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8089\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.8229166666666666\n","TP Rate:  66\n","FP Rate:  1\n","Precision:  0.9850746268656716\n","Recall:  0.8048780487804879\n","F1 Score:  0.8859060402684563\n","MCC:  0.5637736155757088\n","ROC Area:  0.8667247386759582\n","PRC Area:  0.9783096711564131\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - All Feature Config 5\n","### Batch Size : 12\n","### Epochs: 200\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AzcPLrPT0FA","executionInfo":{"status":"ok","timestamp":1649490843765,"user_tz":-330,"elapsed":30139,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"d7774bc5-ea9e-4d4c-ba16-dad5a65c6e1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","32/32 [==============================] - 1s 6ms/step - loss: 0.7169 - accuracy: 0.7565\n","Epoch 2/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.6311 - accuracy: 0.8010\n","Epoch 3/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.5826 - accuracy: 0.7906\n","Epoch 4/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5197 - accuracy: 0.7984\n","Epoch 5/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8063\n","Epoch 6/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3693 - accuracy: 0.8168\n","Epoch 7/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4051 - accuracy: 0.7408\n","Epoch 8/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 0.8037\n","Epoch 9/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2962 - accuracy: 0.8141\n","Epoch 10/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.8194\n","Epoch 11/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.8220\n","Epoch 12/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2104 - accuracy: 0.8220\n","Epoch 13/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.8220\n","Epoch 14/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.8770\n","Epoch 15/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.9686\n","Epoch 16/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9764\n","Epoch 17/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1729 - accuracy: 0.9791\n","Epoch 18/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.9817\n","Epoch 19/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2780 - accuracy: 0.8927\n","Epoch 20/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1905 - accuracy: 0.9634\n","Epoch 21/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9791\n","Epoch 22/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9791\n","Epoch 23/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.9791\n","Epoch 24/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 0.9817\n","Epoch 25/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9476\n","Epoch 26/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9817\n","Epoch 27/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9843\n","Epoch 28/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9791\n","Epoch 29/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9791\n","Epoch 30/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1580 - accuracy: 0.9817\n","Epoch 31/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9817\n","Epoch 32/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9817\n","Epoch 33/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.9843\n","Epoch 34/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1487 - accuracy: 0.9764\n","Epoch 35/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9843\n","Epoch 36/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9791\n","Epoch 37/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9817\n","Epoch 38/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9817\n","Epoch 39/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9686\n","Epoch 40/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9791\n","Epoch 41/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9791\n","Epoch 42/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9476\n","Epoch 43/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9738\n","Epoch 44/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1707 - accuracy: 0.9791\n","Epoch 45/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1551 - accuracy: 0.9764\n","Epoch 46/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1497 - accuracy: 0.9764\n","Epoch 47/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9817\n","Epoch 48/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9817\n","Epoch 49/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9791\n","Epoch 50/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9817\n","Epoch 51/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.9791\n","Epoch 52/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1639 - accuracy: 0.9738\n","Epoch 53/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9686\n","Epoch 54/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9817\n","Epoch 55/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1646 - accuracy: 0.9817\n","Epoch 56/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1522 - accuracy: 0.9791\n","Epoch 57/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9791\n","Epoch 58/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1683 - accuracy: 0.9791\n","Epoch 59/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9791\n","Epoch 60/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1651 - accuracy: 0.9791\n","Epoch 61/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.9791\n","Epoch 62/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1625 - accuracy: 0.9817\n","Epoch 63/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1693 - accuracy: 0.9817\n","Epoch 64/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9791\n","Epoch 65/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9817\n","Epoch 66/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9607\n","Epoch 67/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9791\n","Epoch 68/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9817\n","Epoch 69/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9817\n","Epoch 70/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9791\n","Epoch 71/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9843\n","Epoch 72/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9843\n","Epoch 73/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 0.9712\n","Epoch 74/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9817\n","Epoch 75/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9843\n","Epoch 76/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1398 - accuracy: 0.9843\n","Epoch 77/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9843\n","Epoch 78/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9791\n","Epoch 79/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9817\n","Epoch 80/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9843\n","Epoch 81/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9843\n","Epoch 82/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9843\n","Epoch 83/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9843\n","Epoch 84/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9843\n","Epoch 85/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9843\n","Epoch 86/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1604 - accuracy: 0.9764\n","Epoch 87/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9817\n","Epoch 88/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9817\n","Epoch 89/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9791\n","Epoch 90/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.9764\n","Epoch 91/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9764\n","Epoch 92/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9791\n","Epoch 93/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9843\n","Epoch 94/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9817\n","Epoch 95/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9843\n","Epoch 96/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9843\n","Epoch 97/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9843\n","Epoch 98/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9817\n","Epoch 99/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9712\n","Epoch 100/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.9843\n","Epoch 101/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9843\n","Epoch 102/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.9843\n","Epoch 103/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9764\n","Epoch 104/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9843\n","Epoch 105/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9843\n","Epoch 106/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9843\n","Epoch 107/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9843\n","Epoch 108/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9843\n","Epoch 109/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9817\n","Epoch 110/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9843\n","Epoch 111/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1427 - accuracy: 0.9817\n","Epoch 112/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9843\n","Epoch 113/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9843\n","Epoch 114/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9843\n","Epoch 115/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9843\n","Epoch 116/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9843\n","Epoch 117/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9843\n","Epoch 118/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9843\n","Epoch 119/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9843\n","Epoch 120/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9843\n","Epoch 121/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9764\n","Epoch 122/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9843\n","Epoch 123/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9843\n","Epoch 124/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9843\n","Epoch 125/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.9791\n","Epoch 126/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1400 - accuracy: 0.9843\n","Epoch 127/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.9843\n","Epoch 128/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9843\n","Epoch 129/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9843\n","Epoch 130/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1273 - accuracy: 0.9869\n","Epoch 131/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1902 - accuracy: 0.9660\n","Epoch 132/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1642 - accuracy: 0.9843\n","Epoch 133/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9843\n","Epoch 134/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.9791\n","Epoch 135/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1170 - accuracy: 0.9869\n","Epoch 136/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9869\n","Epoch 137/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9869\n","Epoch 138/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9869\n","Epoch 139/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9869\n","Epoch 140/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9869\n","Epoch 141/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9869\n","Epoch 142/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9843\n","Epoch 143/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9843\n","Epoch 144/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9843\n","Epoch 145/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9869\n","Epoch 146/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.9738\n","Epoch 147/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9372\n","Epoch 148/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9869\n","Epoch 149/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9869\n","Epoch 150/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9869\n","Epoch 151/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9869\n","Epoch 152/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.9869\n","Epoch 153/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9869\n","Epoch 154/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9764\n","Epoch 155/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9895\n","Epoch 156/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9869\n","Epoch 157/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9869\n","Epoch 158/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9791\n","Epoch 159/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9869\n","Epoch 160/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9843\n","Epoch 161/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9895\n","Epoch 162/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.9895\n","Epoch 163/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9895\n","Epoch 164/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9895\n","Epoch 165/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9895\n","Epoch 166/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9895\n","Epoch 167/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9869\n","Epoch 168/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9921\n","Epoch 169/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9895\n","Epoch 170/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9895\n","Epoch 171/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.9817\n","Epoch 172/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9895\n","Epoch 173/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9817\n","Epoch 174/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.9843\n","Epoch 175/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9895\n","Epoch 176/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0991 - accuracy: 0.9895\n","Epoch 177/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9817\n","Epoch 178/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9764\n","Epoch 179/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9817\n","Epoch 180/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9791\n","Epoch 181/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9843\n","Epoch 182/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9791\n","Epoch 183/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9869\n","Epoch 184/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9764\n","Epoch 185/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9738\n","Epoch 186/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9791\n","Epoch 187/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1406 - accuracy: 0.9921\n","Epoch 188/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9895\n","Epoch 189/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9424\n","Epoch 190/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9895\n","Epoch 191/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9895\n","Epoch 192/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0941 - accuracy: 0.9895\n","Epoch 193/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9895\n","Epoch 194/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3062 - accuracy: 0.9764\n","Epoch 195/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.9738\n","Epoch 196/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1276 - accuracy: 0.9660\n","Epoch 197/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9764\n","Epoch 198/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.0949 - accuracy: 0.9791\n","Epoch 199/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9712\n","Epoch 200/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9843\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9583333333333334\n","TP Rate:  79\n","FP Rate:  1\n","Precision:  0.9875\n","Recall:  0.9634146341463414\n","F1 Score:  0.9753086419753086\n","MCC:  0.8447418837575273\n","ROC Area:  0.945993031358885\n","PRC Area:  0.9910823170731707\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Extracting Top 50 Features based on Information Gain"],"metadata":{"id":"FZDM-Zl0Uplz"}},{"cell_type":"code","source":["listTop50 = [\n","'No. of 3 or more input gates',\n","'No. of XOR',\n","'P0 >= 0.9',\n","'P0 >= 0.8',\n","'p0 >= 0.4',\n","'p0 >= 0.3',\n","'p0 >= 0.2',\n","'p0 >= 0.1',\n","'P1 >= 0.6',\n","'SUM P0 * SUM C0',\n","'NORMALISED P0',\n","'NORMALISED P1',\n","'NORMALISED CC0',\n","'HARMONIC MEAN CC1',\n","'SUM CC0*SUM P1',\n","'SUM CC0 * SUM CC1 * SUM CO',\n","'Harmonic Mean CCS',\n","'Geometric Mean CCS',\n","'Harmonic Mean fan_in_1',\n","'Geometric Mean fan_in_1',\n","'fan_in_1 1 - 5',\n","'Geometric Mean fan_in_2',\n","'fan_in_2 5 - 10',\n","'Average fan_in_3',\n","'Normalised fan_in_3',\n","'Geometric Mean fan_in_3',\n","'fan_in_3 1 - 5',\n","'Average fan_in_4',\n","'Normalised fan_in_4',\n","'Standard Deviation fan_in_4',\n","'Population Standard Deviation fan_in_4',\n","'Population Variance fan_in_4',\n","'Variance fan_in_4',\n","'Geometric Mean fan_in_4',\n","'Average fan_in_5',\n","'Normalised fan_in_5',\n","'Standard Deviation fan_in_5',\n","'Population Standard Deviation fan_in_5',\n","'Population Variance fan_in_5',\n","'Variance fan_in_5',\n","'Geometric Mean fan_in_5',\n","'Harmonic Mean in_nearest_pin',\n","'in_nearest_pin 1 - 5',\n","'out_nearest_pout 5 - 10',\n","\"Variance in_ff_1\",\n","\"Sum in_ff_4\",\n","\"Sum in_nearest_ff\",\n","\"in_nearest_ff 1 - 5\",\n","\"in_nearest_ff > 100\",\n","\"Sum out_nearest_ff\",\n","'Class'\n","]"],"metadata":{"id":"EDhkFqP5Uv2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","for name, values in data.iteritems():\n","  if(name.strip() not in listTop50):\n","    if(name.strip() == \"Class\"):\n","      print(\"Class Popped\")\n","    data.pop(name)\n","\n","X = data.iloc[:, :-1].values\n","y = data.iloc[:, -1].values\n","\n","data.to_csv('top_50_features.csv', index = False, encoding='utf-8')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n"],"metadata":{"id":"slRq7Iu9X_MI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"KiU2TU_gWIGg","executionInfo":{"status":"ok","timestamp":1656437151536,"user_tz":-330,"elapsed":536,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"bd668fa2-801c-49ea-ce57-15649641fcf3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   No. of 3 or more input gates  No. of XOR  P0 >= 0.9  P0 >= 0.8  p0 >= 0.4  \\\n","0                         55232        1728     117136     144528     202584   \n","1                             8           0          0          1         22   \n","2                             4           0          0          1         12   \n","3                            20           0          0          0         79   \n","4                            53           0         22         26        333   \n","\n","   p0 >= 0.3  p0 >= 0.2  p0 >= 0.1  P1 >= 0.6  SUM P0 * SUM C0  ...  \\\n","0     205024     219408     242736     114024     3.430000e+12  ...   \n","1         26         37         43         23     7.702381e+04  ...   \n","2         18         24         26         14     8.086893e+04  ...   \n","3         80        122        144         73     3.171978e+06  ...   \n","4        382        590        652        385     2.010000e+16  ...   \n","\n","   Harmonic Mean in_nearest_pin  in_nearest_pin 1 - 5  \\\n","0                      2.992876                286344   \n","1                      2.431181                    34   \n","2                      2.071713                    20   \n","3                      5.792680                    26   \n","4                      2.652446                   460   \n","\n","   out_nearest_pout 5 - 10  Variance in_ff_1  Sum in_ff_4  Sum in_nearest_ff  \\\n","0                    10932          0.020775       373576            1378520   \n","1                       18          0.101010           31              10050   \n","2                       14          0.135385           19               1029   \n","3                       48          0.159463          200                169   \n","4                      330          0.083589          450              27925   \n","\n","   in_nearest_ff 1 - 5  in_nearest_ff > 100  Sum out_nearest_ff  Class  \n","0               307640                  640             1575520      0  \n","1                   30                   10                2070      0  \n","2                   21                    1                1039      0  \n","3                  122                    0                 235      0  \n","4                  491                   26                2872      0  \n","\n","[5 rows x 51 columns]"],"text/html":["\n","  <div id=\"df-dc9faefb-b171-4409-96e2-a886647d9092\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No. of 3 or more input gates</th>\n","      <th>No. of XOR</th>\n","      <th>P0 &gt;= 0.9</th>\n","      <th>P0 &gt;= 0.8</th>\n","      <th>p0 &gt;= 0.4</th>\n","      <th>p0 &gt;= 0.3</th>\n","      <th>p0 &gt;= 0.2</th>\n","      <th>p0 &gt;= 0.1</th>\n","      <th>P1 &gt;= 0.6</th>\n","      <th>SUM P0 * SUM C0</th>\n","      <th>...</th>\n","      <th>Harmonic Mean in_nearest_pin</th>\n","      <th>in_nearest_pin 1 - 5</th>\n","      <th>out_nearest_pout 5 - 10</th>\n","      <th>Variance in_ff_1</th>\n","      <th>Sum in_ff_4</th>\n","      <th>Sum in_nearest_ff</th>\n","      <th>in_nearest_ff 1 - 5</th>\n","      <th>in_nearest_ff &gt; 100</th>\n","      <th>Sum out_nearest_ff</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>55232</td>\n","      <td>1728</td>\n","      <td>117136</td>\n","      <td>144528</td>\n","      <td>202584</td>\n","      <td>205024</td>\n","      <td>219408</td>\n","      <td>242736</td>\n","      <td>114024</td>\n","      <td>3.430000e+12</td>\n","      <td>...</td>\n","      <td>2.992876</td>\n","      <td>286344</td>\n","      <td>10932</td>\n","      <td>0.020775</td>\n","      <td>373576</td>\n","      <td>1378520</td>\n","      <td>307640</td>\n","      <td>640</td>\n","      <td>1575520</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>37</td>\n","      <td>43</td>\n","      <td>23</td>\n","      <td>7.702381e+04</td>\n","      <td>...</td>\n","      <td>2.431181</td>\n","      <td>34</td>\n","      <td>18</td>\n","      <td>0.101010</td>\n","      <td>31</td>\n","      <td>10050</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>2070</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>18</td>\n","      <td>24</td>\n","      <td>26</td>\n","      <td>14</td>\n","      <td>8.086893e+04</td>\n","      <td>...</td>\n","      <td>2.071713</td>\n","      <td>20</td>\n","      <td>14</td>\n","      <td>0.135385</td>\n","      <td>19</td>\n","      <td>1029</td>\n","      <td>21</td>\n","      <td>1</td>\n","      <td>1039</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>79</td>\n","      <td>80</td>\n","      <td>122</td>\n","      <td>144</td>\n","      <td>73</td>\n","      <td>3.171978e+06</td>\n","      <td>...</td>\n","      <td>5.792680</td>\n","      <td>26</td>\n","      <td>48</td>\n","      <td>0.159463</td>\n","      <td>200</td>\n","      <td>169</td>\n","      <td>122</td>\n","      <td>0</td>\n","      <td>235</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>53</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>333</td>\n","      <td>382</td>\n","      <td>590</td>\n","      <td>652</td>\n","      <td>385</td>\n","      <td>2.010000e+16</td>\n","      <td>...</td>\n","      <td>2.652446</td>\n","      <td>460</td>\n","      <td>330</td>\n","      <td>0.083589</td>\n","      <td>450</td>\n","      <td>27925</td>\n","      <td>491</td>\n","      <td>26</td>\n","      <td>2872</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 51 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc9faefb-b171-4409-96e2-a886647d9092')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dc9faefb-b171-4409-96e2-a886647d9092 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dc9faefb-b171-4409-96e2-a886647d9092');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["Testing Multiple Configurations of Deep Neural Network for the extracted features"],"metadata":{"id":"rv7VZZsheYkz"}},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 1\n","### Batch Size : 32\n","### Epochs: 24\n","### L1: 20    \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvMQNVuuYkMG","executionInfo":{"status":"ok","timestamp":1649492791792,"user_tz":-330,"elapsed":5269,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"b4a5a344-4108-469b-a78e-2ce356813cac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","12/12 [==============================] - 2s 5ms/step - loss: 0.6335 - accuracy: 0.1545\n","Epoch 2/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5785 - accuracy: 0.1885\n","Epoch 3/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5352 - accuracy: 0.6021\n","Epoch 4/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5003 - accuracy: 0.6047\n","Epoch 5/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7330\n","Epoch 6/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4524 - accuracy: 0.8717\n","Epoch 7/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.8822\n","Epoch 8/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4170 - accuracy: 0.8822\n","Epoch 9/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.3978 - accuracy: 0.8874\n","Epoch 10/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.3817 - accuracy: 0.9372\n","Epoch 11/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.9660\n","Epoch 12/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.9738\n","Epoch 13/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.9738\n","Epoch 14/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2805 - accuracy: 0.9712\n","Epoch 15/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2570 - accuracy: 0.9660\n","Epoch 16/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2538 - accuracy: 0.9738\n","Epoch 17/24\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2453 - accuracy: 0.9660\n","Epoch 18/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2420 - accuracy: 0.9634\n","Epoch 19/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2388 - accuracy: 0.9660\n","Epoch 20/24\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2417 - accuracy: 0.9660\n","Epoch 21/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2393 - accuracy: 0.9660\n","Epoch 22/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2359 - accuracy: 0.9660\n","Epoch 23/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2364 - accuracy: 0.9660\n","Epoch 24/24\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2357 - accuracy: 0.9660\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9791666666666666\n","TP Rate:  81\n","FP Rate:  1\n","Precision:  0.9878048780487805\n","Recall:  0.9878048780487805\n","F1 Score:  0.9878048780487805\n","MCC:  0.9163763066202091\n","ROC Area:  0.9581881533101045\n","PRC Area:  0.9930132113821138\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 2\n","### Batch Size : 32\n","### Epochs: 200\n","### L1: 20 \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","FN = CM[1][0]\n","TN = CM[0][0]\n","\n","TPR = TP / (TP + FN)\n","FPR = FP / (FP + TN)\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TPR)\n","print(\"FP Rate: \", FPR)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doGNg4jhcXsJ","executionInfo":{"status":"ok","timestamp":1649493041224,"user_tz":-330,"elapsed":11210,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"771b4c79-cabf-4099-b5df-d315acfe83bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","12/12 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.3613\n","Epoch 2/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.6344 - accuracy: 0.3927\n","Epoch 3/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.6004 - accuracy: 0.4372\n","Epoch 4/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.5589 - accuracy: 0.6073\n","Epoch 5/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5179 - accuracy: 0.8010\n","Epoch 6/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.4789 - accuracy: 0.8010\n","Epoch 7/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8508\n","Epoch 8/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8848\n","Epoch 9/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3645 - accuracy: 0.8927\n","Epoch 10/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8901\n","Epoch 11/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8927\n","Epoch 12/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3063 - accuracy: 0.8901\n","Epoch 13/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2920 - accuracy: 0.8874\n","Epoch 14/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.9607\n","Epoch 15/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.9712\n","Epoch 16/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2644 - accuracy: 0.9712\n","Epoch 17/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.9660\n","Epoch 18/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.9660\n","Epoch 19/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.9660\n","Epoch 20/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9660\n","Epoch 21/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9660\n","Epoch 22/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9660\n","Epoch 23/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9660\n","Epoch 24/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9660\n","Epoch 25/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9660\n","Epoch 26/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2343 - accuracy: 0.9660\n","Epoch 27/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9660\n","Epoch 28/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9660\n","Epoch 29/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.9660\n","Epoch 30/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2278 - accuracy: 0.9660\n","Epoch 31/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2312 - accuracy: 0.9660\n","Epoch 32/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9660\n","Epoch 33/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2283 - accuracy: 0.9660\n","Epoch 34/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.9660\n","Epoch 35/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9660\n","Epoch 36/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9660\n","Epoch 37/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9686\n","Epoch 38/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9660\n","Epoch 39/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.9634\n","Epoch 40/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.9686\n","Epoch 41/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9660\n","Epoch 42/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2218 - accuracy: 0.9686\n","Epoch 43/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2315 - accuracy: 0.9660\n","Epoch 44/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2277 - accuracy: 0.9660\n","Epoch 45/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9660\n","Epoch 46/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9686\n","Epoch 47/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9660\n","Epoch 48/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2246 - accuracy: 0.9634\n","Epoch 49/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9686\n","Epoch 50/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2277 - accuracy: 0.9712\n","Epoch 51/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9660\n","Epoch 52/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9660\n","Epoch 53/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9660\n","Epoch 54/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9686\n","Epoch 55/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2285 - accuracy: 0.9660\n","Epoch 56/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9660\n","Epoch 57/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.9660\n","Epoch 58/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9660\n","Epoch 59/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9686\n","Epoch 60/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9686\n","Epoch 61/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9660\n","Epoch 62/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9686\n","Epoch 63/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9634\n","Epoch 64/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9686\n","Epoch 65/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2232 - accuracy: 0.9660\n","Epoch 66/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9634\n","Epoch 67/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2167 - accuracy: 0.9660\n","Epoch 68/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.9634\n","Epoch 69/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9634\n","Epoch 70/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2292 - accuracy: 0.9660\n","Epoch 71/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9607\n","Epoch 72/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9660\n","Epoch 73/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9686\n","Epoch 74/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2167 - accuracy: 0.9660\n","Epoch 75/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9686\n","Epoch 76/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9686\n","Epoch 77/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9686\n","Epoch 78/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9686\n","Epoch 79/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9660\n","Epoch 80/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9686\n","Epoch 81/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9634\n","Epoch 82/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9686\n","Epoch 83/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9686\n","Epoch 84/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9686\n","Epoch 85/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9634\n","Epoch 86/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.9634\n","Epoch 87/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9660\n","Epoch 88/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9686\n","Epoch 89/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9686\n","Epoch 90/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9686\n","Epoch 91/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.9660\n","Epoch 92/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9660\n","Epoch 93/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2118 - accuracy: 0.9686\n","Epoch 94/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9634\n","Epoch 95/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2127 - accuracy: 0.9660\n","Epoch 96/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9686\n","Epoch 97/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9660\n","Epoch 98/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9660\n","Epoch 99/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9634\n","Epoch 100/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9660\n","Epoch 101/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9686\n","Epoch 102/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9660\n","Epoch 103/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9686\n","Epoch 104/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9660\n","Epoch 105/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9634\n","Epoch 106/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9660\n","Epoch 107/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9607\n","Epoch 108/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9686\n","Epoch 109/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9686\n","Epoch 110/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9634\n","Epoch 111/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9686\n","Epoch 112/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9686\n","Epoch 113/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9686\n","Epoch 114/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9634\n","Epoch 115/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9660\n","Epoch 116/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9660\n","Epoch 117/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9660\n","Epoch 118/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9660\n","Epoch 119/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9660\n","Epoch 120/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9634\n","Epoch 121/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9634\n","Epoch 122/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9634\n","Epoch 123/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9660\n","Epoch 124/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9660\n","Epoch 125/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9686\n","Epoch 126/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9686\n","Epoch 127/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9686\n","Epoch 128/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9634\n","Epoch 129/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.9660\n","Epoch 130/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9581\n","Epoch 131/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9686\n","Epoch 132/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9686\n","Epoch 133/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9660\n","Epoch 134/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9686\n","Epoch 135/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9686\n","Epoch 136/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9686\n","Epoch 137/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9686\n","Epoch 138/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9660\n","Epoch 139/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9686\n","Epoch 140/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9686\n","Epoch 141/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9686\n","Epoch 142/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9686\n","Epoch 143/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9686\n","Epoch 144/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9686\n","Epoch 145/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9686\n","Epoch 146/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9686\n","Epoch 147/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9686\n","Epoch 148/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2018 - accuracy: 0.9634\n","Epoch 149/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2023 - accuracy: 0.9686\n","Epoch 150/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9686\n","Epoch 151/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9686\n","Epoch 152/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9686\n","Epoch 153/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9686\n","Epoch 154/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9686\n","Epoch 155/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9686\n","Epoch 156/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9686\n","Epoch 157/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9686\n","Epoch 158/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9634\n","Epoch 159/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.9660\n","Epoch 160/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9581\n","Epoch 161/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9634\n","Epoch 162/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9660\n","Epoch 163/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9555\n","Epoch 164/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9555\n","Epoch 165/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9634\n","Epoch 166/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9686\n","Epoch 167/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9686\n","Epoch 168/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9686\n","Epoch 169/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9686\n","Epoch 170/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9686\n","Epoch 171/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.9686\n","Epoch 172/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.9686\n","Epoch 173/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1955 - accuracy: 0.9686\n","Epoch 174/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9686\n","Epoch 175/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9686\n","Epoch 176/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9686\n","Epoch 177/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9686\n","Epoch 178/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9686\n","Epoch 179/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9686\n","Epoch 180/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1939 - accuracy: 0.9686\n","Epoch 181/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9686\n","Epoch 182/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9686\n","Epoch 183/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.9686\n","Epoch 184/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1877 - accuracy: 0.9660\n","Epoch 185/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9686\n","Epoch 186/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.9686\n","Epoch 187/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1905 - accuracy: 0.9686\n","Epoch 188/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9686\n","Epoch 189/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9686\n","Epoch 190/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9686\n","Epoch 191/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9686\n","Epoch 192/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9686\n","Epoch 193/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1888 - accuracy: 0.9686\n","Epoch 194/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9686\n","Epoch 195/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9686\n","Epoch 196/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9686\n","Epoch 197/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1971 - accuracy: 0.9660\n","Epoch 198/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1893 - accuracy: 0.9686\n","Epoch 199/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9660\n","Epoch 200/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9660\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9791666666666666\n","TP Rate:  81\n","FP Rate:  1\n","Precision:  0.9878048780487805\n","Recall:  0.9878048780487805\n","F1 Score:  0.9878048780487805\n","MCC:  0.9163763066202091\n","ROC Area:  0.9581881533101045\n","PRC Area:  0.9930132113821138\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 3\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 10\n","### L5: 20\n","\n","import pickle\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=10, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","filename = 'finalized_model.sav'\n","pickle.dump(ann, open(filename, 'wb'))\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vimF6CTcvaq","executionInfo":{"status":"ok","timestamp":1656437213077,"user_tz":-330,"elapsed":4490,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"eb7889f0-3880-444d-cbef-00689eb556f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 2ms/step - loss: 0.6786 - accuracy: 0.2094\n","Epoch 2/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.6265 - accuracy: 0.3822\n","Epoch 3/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.6126\n","Epoch 4/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8168\n","Epoch 5/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8743\n","Epoch 6/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8691\n","Epoch 7/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8901\n","Epoch 8/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8822\n","Epoch 9/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8822\n","Epoch 10/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8901\n","Epoch 11/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8770\n","Epoch 12/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.9162\n","Epoch 13/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.9529\n","Epoch 14/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.9162\n","Epoch 15/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.9503\n","Epoch 16/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.9241\n","Epoch 17/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.9555\n","Epoch 18/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.9529\n","Epoch 19/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.9634\n","Epoch 20/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.9607\n","Epoch 21/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2688 - accuracy: 0.9634\n","Epoch 22/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9634\n","Epoch 23/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.9581\n","Epoch 24/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.9634\n","INFO:tensorflow:Assets written to: ram://01050c9a-d0c4-4315-8a8d-03f97c27fe5e/assets\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9895833333333334\n","TP Rate:  82\n","FP Rate:  1\n","Precision:  0.9879518072289156\n","Recall:  1.0\n","F1 Score:  0.993939393939394\n","MCC:  0.9578015561682279\n","ROC Area:  0.9642857142857143\n","PRC Area:  0.9939759036144578\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["#Load Model and create predictions\n","\n","data_new = pd.read_csv (r'top_50_features.csv')\n","data_new.head()\n","\n","X = data_new.iloc[:, 1:-1].values\n","y = data_new.iloc[:, -1].values\n","\n","sc = StandardScaler()\n","X = sc.fit_transform(X)\n","\n","loaded_model = pickle.load(open(filename, 'rb'))\n","y_pred = loaded_model.predict(X)\n","\n","y_pred = (y_pred > 0.5)\n","\n","list_of_tuples = list(zip(y_pred, y_test))\n"," \n","# Converting lists of tuples into\n","# pandas Dataframe.\n","\n","data_new[\"Prediction\"] = y_pred\n","\n","data_new.replace({False: \"Non Trojan\", True: \"Trojan\"}, inplace=True)\n","\n","# data_new['Prediction'] = data_new['Prediction'].map({0:'No Trojan', 1:'Trojan'})\n","\n","data_new.to_csv('result.csv', index = False, encoding='utf-8')\n","\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y, y_pred))\n","print(\"Recall: \", metrics.recall_score(y, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n","\n","# # print(\"Enter the Label (Trojan = 1, No Trojan = 0)\")\n","\n","\n","\n","# data_new\n","# data = {'Prediction': y_pred,}\n","# fdf = pd.DataFrame(data, )\n","# fdf.head()\n","# print(loaded_model.evaluate(X_train, y_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"svP49Qk82qFm","executionInfo":{"status":"ok","timestamp":1656433516885,"user_tz":-330,"elapsed":1290,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"355de77b-85cd-4d9e-e1f4-857f044f7738"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   No. of 3 or more input gates  No. of XOR  P0 >= 0.9  P0 >= 0.8  p0 >= 0.4  \\\n","0                         55232        1728     117136     144528     202584   \n","1                             8           0          0          1         22   \n","2                             4           0          0          1         12   \n","3                            20           0          0          0         79   \n","4                            53           0         22         26        333   \n","\n","   p0 >= 0.3  p0 >= 0.2  p0 >= 0.1  P1 >= 0.6  SUM P0 * SUM C0  ...  \\\n","0     205024     219408     242736     114024     3.430000e+12  ...   \n","1         26         37         43         23     7.702381e+04  ...   \n","2         18         24         26         14     8.086893e+04  ...   \n","3         80        122        144         73     3.171978e+06  ...   \n","4        382        590        652        385     2.010000e+16  ...   \n","\n","   in_nearest_pin 1 - 5  out_nearest_pout 5 - 10  Variance in_ff_1  \\\n","0                286344                    10932          0.020775   \n","1                    34                       18          0.101010   \n","2                    20                       14          0.135385   \n","3                    26                       48          0.159463   \n","4                   460                      330          0.083589   \n","\n","   Sum in_ff_4  Sum in_nearest_ff  in_nearest_ff 1 - 5  in_nearest_ff > 100  \\\n","0       373576            1378520               307640                  640   \n","1           31              10050                   30                   10   \n","2           19               1029                   21                    1   \n","3          200                169                  122                    0   \n","4          450              27925                  491                   26   \n","\n","   Sum out_nearest_ff  Class  Prediction  \n","0             1575520      0      Trojan  \n","1                2070      0  Non Trojan  \n","2                1039      0  Non Trojan  \n","3                 235      0  Non Trojan  \n","4                2872      0  Non Trojan  \n","\n","[5 rows x 52 columns]"],"text/html":["\n","  <div id=\"df-e81397f5-ddc4-4814-8a89-3f64c260066f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No. of 3 or more input gates</th>\n","      <th>No. of XOR</th>\n","      <th>P0 &gt;= 0.9</th>\n","      <th>P0 &gt;= 0.8</th>\n","      <th>p0 &gt;= 0.4</th>\n","      <th>p0 &gt;= 0.3</th>\n","      <th>p0 &gt;= 0.2</th>\n","      <th>p0 &gt;= 0.1</th>\n","      <th>P1 &gt;= 0.6</th>\n","      <th>SUM P0 * SUM C0</th>\n","      <th>...</th>\n","      <th>in_nearest_pin 1 - 5</th>\n","      <th>out_nearest_pout 5 - 10</th>\n","      <th>Variance in_ff_1</th>\n","      <th>Sum in_ff_4</th>\n","      <th>Sum in_nearest_ff</th>\n","      <th>in_nearest_ff 1 - 5</th>\n","      <th>in_nearest_ff &gt; 100</th>\n","      <th>Sum out_nearest_ff</th>\n","      <th>Class</th>\n","      <th>Prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>55232</td>\n","      <td>1728</td>\n","      <td>117136</td>\n","      <td>144528</td>\n","      <td>202584</td>\n","      <td>205024</td>\n","      <td>219408</td>\n","      <td>242736</td>\n","      <td>114024</td>\n","      <td>3.430000e+12</td>\n","      <td>...</td>\n","      <td>286344</td>\n","      <td>10932</td>\n","      <td>0.020775</td>\n","      <td>373576</td>\n","      <td>1378520</td>\n","      <td>307640</td>\n","      <td>640</td>\n","      <td>1575520</td>\n","      <td>0</td>\n","      <td>Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>37</td>\n","      <td>43</td>\n","      <td>23</td>\n","      <td>7.702381e+04</td>\n","      <td>...</td>\n","      <td>34</td>\n","      <td>18</td>\n","      <td>0.101010</td>\n","      <td>31</td>\n","      <td>10050</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>2070</td>\n","      <td>0</td>\n","      <td>Non Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>18</td>\n","      <td>24</td>\n","      <td>26</td>\n","      <td>14</td>\n","      <td>8.086893e+04</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>14</td>\n","      <td>0.135385</td>\n","      <td>19</td>\n","      <td>1029</td>\n","      <td>21</td>\n","      <td>1</td>\n","      <td>1039</td>\n","      <td>0</td>\n","      <td>Non Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>79</td>\n","      <td>80</td>\n","      <td>122</td>\n","      <td>144</td>\n","      <td>73</td>\n","      <td>3.171978e+06</td>\n","      <td>...</td>\n","      <td>26</td>\n","      <td>48</td>\n","      <td>0.159463</td>\n","      <td>200</td>\n","      <td>169</td>\n","      <td>122</td>\n","      <td>0</td>\n","      <td>235</td>\n","      <td>0</td>\n","      <td>Non Trojan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>53</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>333</td>\n","      <td>382</td>\n","      <td>590</td>\n","      <td>652</td>\n","      <td>385</td>\n","      <td>2.010000e+16</td>\n","      <td>...</td>\n","      <td>460</td>\n","      <td>330</td>\n","      <td>0.083589</td>\n","      <td>450</td>\n","      <td>27925</td>\n","      <td>491</td>\n","      <td>26</td>\n","      <td>2872</td>\n","      <td>0</td>\n","      <td>Non Trojan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 52 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e81397f5-ddc4-4814-8a89-3f64c260066f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e81397f5-ddc4-4814-8a89-3f64c260066f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e81397f5-ddc4-4814-8a89-3f64c260066f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 4\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG7medlXdB0g","executionInfo":{"status":"ok","timestamp":1649493196874,"user_tz":-330,"elapsed":4513,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"2fa3e9c8-64ee-446d-afcc-1183d57cab20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 4ms/step - loss: 0.6622 - accuracy: 0.2539\n","Epoch 2/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6125 - accuracy: 0.4162\n","Epoch 3/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5539 - accuracy: 0.6990\n","Epoch 4/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4864 - accuracy: 0.8220\n","Epoch 5/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.8848\n","Epoch 6/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8717\n","Epoch 7/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.9581\n","Epoch 8/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4109 - accuracy: 0.9110\n","Epoch 9/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4008 - accuracy: 0.9424\n","Epoch 10/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3917 - accuracy: 0.9607\n","Epoch 11/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.9581\n","Epoch 12/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3670 - accuracy: 0.9529\n","Epoch 13/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.9712\n","Epoch 14/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3564 - accuracy: 0.9712\n","Epoch 15/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3524 - accuracy: 0.9712\n","Epoch 16/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.9686\n","Epoch 17/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3442 - accuracy: 0.9712\n","Epoch 18/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.9712\n","Epoch 19/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.9712\n","Epoch 20/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.9712\n","Epoch 21/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.9712\n","Epoch 22/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3280 - accuracy: 0.9686\n","Epoch 23/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3252 - accuracy: 0.9712\n","Epoch 24/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.9712\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9895833333333334\n","TP Rate:  82\n","FP Rate:  1\n","Precision:  0.9879518072289156\n","Recall:  1.0\n","F1 Score:  0.993939393939394\n","MCC:  0.9578015561682279\n","ROC Area:  0.9642857142857143\n","PRC Area:  0.9939759036144578\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 5\n","### Batch Size : 12\n","### Epochs: 200\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D690EgtkdSx1","executionInfo":{"status":"ok","timestamp":1649493338185,"user_tz":-330,"elapsed":37690,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"e2314848-5712-4131-94b0-ef1696871709"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","32/32 [==============================] - 1s 8ms/step - loss: 0.7030 - accuracy: 0.8455\n","Epoch 2/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.6909 - accuracy: 0.8455\n","Epoch 3/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.6867 - accuracy: 0.8403\n","Epoch 4/200\n","32/32 [==============================] - 0s 11ms/step - loss: 0.6766 - accuracy: 0.8534\n","Epoch 5/200\n","32/32 [==============================] - 0s 9ms/step - loss: 0.6689 - accuracy: 0.8115\n","Epoch 6/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.6522 - accuracy: 0.8770\n","Epoch 7/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.6264 - accuracy: 0.8927\n","Epoch 8/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.6069 - accuracy: 0.9241\n","Epoch 9/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.6043 - accuracy: 0.9372\n","Epoch 10/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5864 - accuracy: 0.9005\n","Epoch 11/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.5751 - accuracy: 0.9136\n","Epoch 12/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5449 - accuracy: 0.9346\n","Epoch 13/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5773 - accuracy: 0.9058\n","Epoch 14/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.8770\n","Epoch 15/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.5255 - accuracy: 0.8691\n","Epoch 16/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5125 - accuracy: 0.8743\n","Epoch 17/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.5290 - accuracy: 0.8613\n","Epoch 18/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.8796\n","Epoch 19/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.5094 - accuracy: 0.8796\n","Epoch 20/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.8796\n","Epoch 21/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.8796\n","Epoch 22/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4895 - accuracy: 0.8822\n","Epoch 23/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.8822\n","Epoch 24/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.8822\n","Epoch 25/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 0.8796\n","Epoch 26/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4751 - accuracy: 0.8822\n","Epoch 27/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4746 - accuracy: 0.8822\n","Epoch 28/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4729 - accuracy: 0.8822\n","Epoch 29/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4706 - accuracy: 0.8822\n","Epoch 30/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4648 - accuracy: 0.8822\n","Epoch 31/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4640 - accuracy: 0.8822\n","Epoch 32/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4656 - accuracy: 0.8822\n","Epoch 33/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4621 - accuracy: 0.8822\n","Epoch 34/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4600 - accuracy: 0.8822\n","Epoch 35/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4559 - accuracy: 0.8822\n","Epoch 36/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4497 - accuracy: 0.8822\n","Epoch 37/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4513 - accuracy: 0.8822\n","Epoch 38/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4488 - accuracy: 0.8822\n","Epoch 39/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4478 - accuracy: 0.8822\n","Epoch 40/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4474 - accuracy: 0.8822\n","Epoch 41/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4514 - accuracy: 0.8770\n","Epoch 42/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4478 - accuracy: 0.8822\n","Epoch 43/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.8822\n","Epoch 44/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4462 - accuracy: 0.8822\n","Epoch 45/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4353 - accuracy: 0.8822\n","Epoch 46/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4373 - accuracy: 0.8822\n","Epoch 47/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4332 - accuracy: 0.8822\n","Epoch 48/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4318 - accuracy: 0.8822\n","Epoch 49/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4326 - accuracy: 0.8822\n","Epoch 50/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.8822\n","Epoch 51/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.8822\n","Epoch 52/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8822\n","Epoch 53/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.8822\n","Epoch 54/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4360 - accuracy: 0.8822\n","Epoch 55/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4232 - accuracy: 0.8822\n","Epoch 56/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4198 - accuracy: 0.8822\n","Epoch 57/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8822\n","Epoch 58/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4189 - accuracy: 0.8822\n","Epoch 59/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4175 - accuracy: 0.8822\n","Epoch 60/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4146 - accuracy: 0.8822\n","Epoch 61/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4162 - accuracy: 0.8822\n","Epoch 62/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4120 - accuracy: 0.8822\n","Epoch 63/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4213 - accuracy: 0.8822\n","Epoch 64/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4197 - accuracy: 0.8848\n","Epoch 65/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4121 - accuracy: 0.8822\n","Epoch 66/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4124 - accuracy: 0.8822\n","Epoch 67/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4115 - accuracy: 0.8822\n","Epoch 68/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4095 - accuracy: 0.8822\n","Epoch 69/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4093 - accuracy: 0.8822\n","Epoch 70/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4096 - accuracy: 0.8822\n","Epoch 71/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4123 - accuracy: 0.8822\n","Epoch 72/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4021 - accuracy: 0.8822\n","Epoch 73/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4077 - accuracy: 0.8822\n","Epoch 74/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4160 - accuracy: 0.8796\n","Epoch 75/200\n","32/32 [==============================] - 0s 8ms/step - loss: 0.4024 - accuracy: 0.8822\n","Epoch 76/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.3971 - accuracy: 0.8822\n","Epoch 77/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4127 - accuracy: 0.8822\n","Epoch 78/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3987 - accuracy: 0.8822\n","Epoch 79/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3976 - accuracy: 0.8822\n","Epoch 80/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.4011 - accuracy: 0.8822\n","Epoch 81/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4008 - accuracy: 0.8822\n","Epoch 82/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3963 - accuracy: 0.8822\n","Epoch 83/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3931 - accuracy: 0.8822\n","Epoch 84/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4047 - accuracy: 0.8822\n","Epoch 85/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3961 - accuracy: 0.8822\n","Epoch 86/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8822\n","Epoch 87/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3957 - accuracy: 0.8822\n","Epoch 88/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4011 - accuracy: 0.8822\n","Epoch 89/200\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3912 - accuracy: 0.8822\n","Epoch 90/200\n","32/32 [==============================] - 0s 7ms/step - loss: 0.3910 - accuracy: 0.8822\n","Epoch 91/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3904 - accuracy: 0.8822\n","Epoch 92/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3914 - accuracy: 0.8822\n","Epoch 93/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3949 - accuracy: 0.8822\n","Epoch 94/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3920 - accuracy: 0.8822\n","Epoch 95/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8717\n","Epoch 96/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8822\n","Epoch 97/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8822\n","Epoch 98/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3887 - accuracy: 0.8822\n","Epoch 99/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.8822\n","Epoch 100/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3871 - accuracy: 0.8822\n","Epoch 101/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3871 - accuracy: 0.8822\n","Epoch 102/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8822\n","Epoch 103/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8822\n","Epoch 104/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8796\n","Epoch 105/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8822\n","Epoch 106/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8822\n","Epoch 107/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3845 - accuracy: 0.8822\n","Epoch 108/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.8822\n","Epoch 109/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3871 - accuracy: 0.8796\n","Epoch 110/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3880 - accuracy: 0.8822\n","Epoch 111/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3830 - accuracy: 0.8822\n","Epoch 112/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8822\n","Epoch 113/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3785 - accuracy: 0.8822\n","Epoch 114/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8822\n","Epoch 115/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3777 - accuracy: 0.8822\n","Epoch 116/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8822\n","Epoch 117/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3955 - accuracy: 0.8822\n","Epoch 118/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3800 - accuracy: 0.8822\n","Epoch 119/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8822\n","Epoch 120/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3807 - accuracy: 0.8822\n","Epoch 121/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8822\n","Epoch 122/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3735 - accuracy: 0.8822\n","Epoch 123/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.8822\n","Epoch 124/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8822\n","Epoch 125/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3726 - accuracy: 0.8822\n","Epoch 126/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.8796\n","Epoch 127/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3735 - accuracy: 0.8796\n","Epoch 128/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3808 - accuracy: 0.8822\n","Epoch 129/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8691\n","Epoch 130/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.8822\n","Epoch 131/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3744 - accuracy: 0.8822\n","Epoch 132/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8822\n","Epoch 133/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8822\n","Epoch 134/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3749 - accuracy: 0.8822\n","Epoch 135/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8822\n","Epoch 136/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3782 - accuracy: 0.8822\n","Epoch 137/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8796\n","Epoch 138/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3937 - accuracy: 0.8822\n","Epoch 139/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.8822\n","Epoch 140/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3699 - accuracy: 0.8822\n","Epoch 141/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8770\n","Epoch 142/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.8822\n","Epoch 143/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3740 - accuracy: 0.8822\n","Epoch 144/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3645 - accuracy: 0.8848\n","Epoch 145/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3749 - accuracy: 0.8822\n","Epoch 146/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3585 - accuracy: 0.8717\n","Epoch 147/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8822\n","Epoch 148/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8822\n","Epoch 149/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8822\n","Epoch 150/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8822\n","Epoch 151/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.8822\n","Epoch 152/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3633 - accuracy: 0.8848\n","Epoch 153/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3748 - accuracy: 0.8822\n","Epoch 154/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8822\n","Epoch 155/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8822\n","Epoch 156/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3582 - accuracy: 0.8822\n","Epoch 157/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.8822\n","Epoch 158/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3635 - accuracy: 0.8822\n","Epoch 159/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8796\n","Epoch 160/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3556 - accuracy: 0.8822\n","Epoch 161/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8822\n","Epoch 162/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8822\n","Epoch 163/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3819 - accuracy: 0.8796\n","Epoch 164/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3555 - accuracy: 0.8822\n","Epoch 165/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.8822\n","Epoch 166/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3533 - accuracy: 0.8822\n","Epoch 167/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8796\n","Epoch 168/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3519 - accuracy: 0.8770\n","Epoch 169/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3765 - accuracy: 0.8822\n","Epoch 170/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8822\n","Epoch 171/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.8822\n","Epoch 172/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3597 - accuracy: 0.8822\n","Epoch 173/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3553 - accuracy: 0.8796\n","Epoch 174/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3555 - accuracy: 0.8822\n","Epoch 175/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3455 - accuracy: 0.8796\n","Epoch 176/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3618 - accuracy: 0.8822\n","Epoch 177/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.8822\n","Epoch 178/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3865 - accuracy: 0.8482\n","Epoch 179/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8743\n","Epoch 180/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.8822\n","Epoch 181/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3642 - accuracy: 0.8822\n","Epoch 182/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8743\n","Epoch 183/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8822\n","Epoch 184/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.8743\n","Epoch 185/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8822\n","Epoch 186/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.8639\n","Epoch 187/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.3782 - accuracy: 0.8822\n","Epoch 188/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3499 - accuracy: 0.8822\n","Epoch 189/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3765 - accuracy: 0.8822\n","Epoch 190/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3640 - accuracy: 0.8796\n","Epoch 191/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3618 - accuracy: 0.8822\n","Epoch 192/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.8796\n","Epoch 193/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3631 - accuracy: 0.8796\n","Epoch 194/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3917 - accuracy: 0.8586\n","Epoch 195/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8822\n","Epoch 196/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8822\n","Epoch 197/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3392 - accuracy: 0.8848\n","Epoch 198/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8770\n","Epoch 199/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3722 - accuracy: 0.8743\n","Epoch 200/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.8848\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.8958333333333334\n","TP Rate:  73\n","FP Rate:  1\n","Precision:  0.9864864864864865\n","Recall:  0.8902439024390244\n","F1 Score:  0.9358974358974359\n","MCC:  0.6875902492248791\n","ROC Area:  0.9094076655052266\n","PRC Area:  0.9852401944627553\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Extracting Top 45 Features based on Information Gain"],"metadata":{"id":"XMR_LNoQeK7I"}},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset_HTPred_classification.csv\",na_values=\"?\")\n","data_string = data.dtypes==object\n","data_cols = data.columns[data_string].tolist()\n","le = LabelEncoder()\n","data[data_cols] = data[data_cols].apply(lambda col: le.fit_transform(col))"],"metadata":{"id":"3ubnKcDweSWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["listTop45 = [\n","'No. of 3 or more input gates',\n","'No. of XOR',\n","'P0 >= 0.9',\n","'P0 >= 0.8',\n","'p0 >= 0.4',\n","'p0 >= 0.3',\n","'p0 >= 0.2',\n","'p0 >= 0.1',\n","'P1 >= 0.6',\n","'SUM P0 * SUM C0',\n","'NORMALISED P0',\n","'NORMALISED P1',\n","'NORMALISED CC0',\n","'HARMONIC MEAN CC1',\n","'SUM CC0*SUM P1',\n","'SUM CC0 * SUM CC1 * SUM CO',\n","'Harmonic Mean CCS',\n","'Geometric Mean CCS',\n","'Harmonic Mean fan_in_1',\n","'Geometric Mean fan_in_1',\n","'fan_in_1 1 - 5',\n","'Geometric Mean fan_in_2',\n","'fan_in_2 5 - 10',\n","'Average fan_in_3',\n","'Normalised fan_in_3',\n","'Geometric Mean fan_in_3',\n","'fan_in_3 1 - 5',\n","'Average fan_in_4',\n","'Normalised fan_in_4',\n","'Standard Deviation fan_in_4',\n","'Population Standard Deviation fan_in_4',\n","'Population Variance fan_in_4',\n","'Variance fan_in_4',\n","'Geometric Mean fan_in_4',\n","'Average fan_in_5',\n","'Normalised fan_in_5',\n","'Standard Deviation fan_in_5',\n","'Population Standard Deviation fan_in_5',\n","'Population Variance fan_in_5',\n","'Variance fan_in_5',\n","'Geometric Mean fan_in_5',\n","'Harmonic Mean in_nearest_pin',\n","'in_nearest_pin 1 - 5',\n","'out_nearest_pout 5 - 10',\n","\"Variance in_ff_1\",\n","'Class'\n","]"],"metadata":{"id":"GRaZMJ22fMLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name, values in data.iteritems():\n","  if(name.strip() not in listTop45):\n","    if(name.strip() == \"Class\"):\n","      print(\"Class Popped\")\n","    data.pop(name)\n","\n","X = data.iloc[:, 1:-1].values\n","y = data.iloc[:, -1].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"-AEZ0DiZfKqR","executionInfo":{"status":"ok","timestamp":1649493765168,"user_tz":-330,"elapsed":1136,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"313b1be8-e9c8-470f-98b4-81fee28bc61b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   No. of 3 or more input gates  No. of XOR  P0 >= 0.9  P0 >= 0.8  p0 >= 0.4  \\\n","0                         55232        1728     117136     144528     202584   \n","1                             8           0          0          1         22   \n","2                             4           0          0          1         12   \n","3                            20           0          0          0         79   \n","4                            53           0         22         26        333   \n","\n","   p0 >= 0.3  p0 >= 0.2  p0 >= 0.1  P1 >= 0.6  SUM P0 * SUM C0  ...  \\\n","0     205024     219408     242736     114024     3.430000e+12  ...   \n","1         26         37         43         23     7.702381e+04  ...   \n","2         18         24         26         14     8.086893e+04  ...   \n","3         80        122        144         73     3.171978e+06  ...   \n","4        382        590        652        385     2.010000e+16  ...   \n","\n","   Standard Deviation fan_in_5  Population Standard Deviation fan_in_5  \\\n","0                    14.057330                               14.057308   \n","1                     9.395733                                9.290749   \n","2                     4.134657                                4.054364   \n","3                    10.221800                               10.188120   \n","4                    10.765379                               10.757880   \n","\n","   Population Variance fan_in_5  Variance fan_in_5  Geometric Mean fan_in_5  \\\n","0                    197.607913         197.608537                13.613545   \n","1                     86.318025          88.279798                14.020861   \n","2                     16.437870          17.095385                 8.812276   \n","3                    103.797784         104.485186                10.802326   \n","4                    115.731978         115.893389                10.545540   \n","\n","   Harmonic Mean in_nearest_pin  in_nearest_pin 1 - 5  \\\n","0                      2.992876                286344   \n","1                      2.431181                    34   \n","2                      2.071713                    20   \n","3                      5.792680                    26   \n","4                      2.652446                   460   \n","\n","   out_nearest_pout 5 - 10  Variance in_ff_1  Class  \n","0                    10932          0.020775      0  \n","1                       18          0.101010      0  \n","2                       14          0.135385      0  \n","3                       48          0.159463      0  \n","4                      330          0.083589      0  \n","\n","[5 rows x 46 columns]"],"text/html":["\n","  <div id=\"df-034ee7d5-ad53-4dd9-933a-bfbe31b6ffe9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No. of 3 or more input gates</th>\n","      <th>No. of XOR</th>\n","      <th>P0 &gt;= 0.9</th>\n","      <th>P0 &gt;= 0.8</th>\n","      <th>p0 &gt;= 0.4</th>\n","      <th>p0 &gt;= 0.3</th>\n","      <th>p0 &gt;= 0.2</th>\n","      <th>p0 &gt;= 0.1</th>\n","      <th>P1 &gt;= 0.6</th>\n","      <th>SUM P0 * SUM C0</th>\n","      <th>...</th>\n","      <th>Standard Deviation fan_in_5</th>\n","      <th>Population Standard Deviation fan_in_5</th>\n","      <th>Population Variance fan_in_5</th>\n","      <th>Variance fan_in_5</th>\n","      <th>Geometric Mean fan_in_5</th>\n","      <th>Harmonic Mean in_nearest_pin</th>\n","      <th>in_nearest_pin 1 - 5</th>\n","      <th>out_nearest_pout 5 - 10</th>\n","      <th>Variance in_ff_1</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>55232</td>\n","      <td>1728</td>\n","      <td>117136</td>\n","      <td>144528</td>\n","      <td>202584</td>\n","      <td>205024</td>\n","      <td>219408</td>\n","      <td>242736</td>\n","      <td>114024</td>\n","      <td>3.430000e+12</td>\n","      <td>...</td>\n","      <td>14.057330</td>\n","      <td>14.057308</td>\n","      <td>197.607913</td>\n","      <td>197.608537</td>\n","      <td>13.613545</td>\n","      <td>2.992876</td>\n","      <td>286344</td>\n","      <td>10932</td>\n","      <td>0.020775</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>37</td>\n","      <td>43</td>\n","      <td>23</td>\n","      <td>7.702381e+04</td>\n","      <td>...</td>\n","      <td>9.395733</td>\n","      <td>9.290749</td>\n","      <td>86.318025</td>\n","      <td>88.279798</td>\n","      <td>14.020861</td>\n","      <td>2.431181</td>\n","      <td>34</td>\n","      <td>18</td>\n","      <td>0.101010</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>18</td>\n","      <td>24</td>\n","      <td>26</td>\n","      <td>14</td>\n","      <td>8.086893e+04</td>\n","      <td>...</td>\n","      <td>4.134657</td>\n","      <td>4.054364</td>\n","      <td>16.437870</td>\n","      <td>17.095385</td>\n","      <td>8.812276</td>\n","      <td>2.071713</td>\n","      <td>20</td>\n","      <td>14</td>\n","      <td>0.135385</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>79</td>\n","      <td>80</td>\n","      <td>122</td>\n","      <td>144</td>\n","      <td>73</td>\n","      <td>3.171978e+06</td>\n","      <td>...</td>\n","      <td>10.221800</td>\n","      <td>10.188120</td>\n","      <td>103.797784</td>\n","      <td>104.485186</td>\n","      <td>10.802326</td>\n","      <td>5.792680</td>\n","      <td>26</td>\n","      <td>48</td>\n","      <td>0.159463</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>53</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>26</td>\n","      <td>333</td>\n","      <td>382</td>\n","      <td>590</td>\n","      <td>652</td>\n","      <td>385</td>\n","      <td>2.010000e+16</td>\n","      <td>...</td>\n","      <td>10.765379</td>\n","      <td>10.757880</td>\n","      <td>115.731978</td>\n","      <td>115.893389</td>\n","      <td>10.545540</td>\n","      <td>2.652446</td>\n","      <td>460</td>\n","      <td>330</td>\n","      <td>0.083589</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 46 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-034ee7d5-ad53-4dd9-933a-bfbe31b6ffe9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-034ee7d5-ad53-4dd9-933a-bfbe31b6ffe9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-034ee7d5-ad53-4dd9-933a-bfbe31b6ffe9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 1\n","### Batch Size : 32\n","### Epochs: 24\n","### L1: 20    \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISq52XH9fJZ8","executionInfo":{"status":"ok","timestamp":1649493780822,"user_tz":-330,"elapsed":2333,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"b00bf38e-75dd-4302-f733-72d2c023a84f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","12/12 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.2618\n","Epoch 2/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.6345 - accuracy: 0.2408\n","Epoch 3/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.3298\n","Epoch 4/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.4529\n","Epoch 5/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.7932\n","Epoch 6/24\n","12/12 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8927\n","Epoch 7/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.8901\n","Epoch 8/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.4178 - accuracy: 0.8874\n","Epoch 9/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8927\n","Epoch 10/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.9005\n","Epoch 11/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.9581\n","Epoch 12/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.9607\n","Epoch 13/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.9634\n","Epoch 14/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2741 - accuracy: 0.9634\n","Epoch 15/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2672 - accuracy: 0.9634\n","Epoch 16/24\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2570 - accuracy: 0.9660\n","Epoch 17/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.9660\n","Epoch 18/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9660\n","Epoch 19/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.9660\n","Epoch 20/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.9660\n","Epoch 21/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.9660\n","Epoch 22/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.9660\n","Epoch 23/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.9660\n","Epoch 24/24\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.9660\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9791666666666666\n","TP Rate:  81\n","FP Rate:  1\n","Precision:  0.9878048780487805\n","Recall:  0.9878048780487805\n","F1 Score:  0.9878048780487805\n","MCC:  0.9163763066202091\n","ROC Area:  0.9581881533101045\n","PRC Area:  0.9930132113821138\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 2\n","### Batch Size : 32\n","### Epochs: 200\n","### L1: 20 \n","### L2: 20\n","### L3: 20\n","### L4: 20\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 32, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhw0kN7gfH5J","executionInfo":{"status":"ok","timestamp":1649493802444,"user_tz":-330,"elapsed":21236,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"aab97df1-e39f-4a14-c2ac-96280cfb1545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","12/12 [==============================] - 1s 7ms/step - loss: 0.6999 - accuracy: 0.8691\n","Epoch 2/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6593 - accuracy: 0.8037\n","Epoch 3/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.6377 - accuracy: 0.6571\n","Epoch 4/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.6172 - accuracy: 0.6283\n","Epoch 5/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5910 - accuracy: 0.8272\n","Epoch 6/200\n","12/12 [==============================] - 0s 8ms/step - loss: 0.5602 - accuracy: 0.8770\n","Epoch 7/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.8743\n","Epoch 8/200\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4611 - accuracy: 0.8717\n","Epoch 9/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.4064 - accuracy: 0.8743\n","Epoch 10/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3851 - accuracy: 0.8848\n","Epoch 11/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3460 - accuracy: 0.8848\n","Epoch 12/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3369 - accuracy: 0.8874\n","Epoch 13/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3401 - accuracy: 0.9555\n","Epoch 14/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.3117 - accuracy: 0.8953\n","Epoch 15/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2951 - accuracy: 0.9607\n","Epoch 16/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2825 - accuracy: 0.9607\n","Epoch 17/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2784 - accuracy: 0.9686\n","Epoch 18/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.9634\n","Epoch 19/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2684 - accuracy: 0.9660\n","Epoch 20/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2542 - accuracy: 0.9660\n","Epoch 21/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2625 - accuracy: 0.9634\n","Epoch 22/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2407 - accuracy: 0.9660\n","Epoch 23/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2599 - accuracy: 0.9607\n","Epoch 24/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9712\n","Epoch 25/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2493 - accuracy: 0.9660\n","Epoch 26/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2383 - accuracy: 0.9660\n","Epoch 27/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2526 - accuracy: 0.9738\n","Epoch 28/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2438 - accuracy: 0.9660\n","Epoch 29/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2402 - accuracy: 0.9660\n","Epoch 30/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2440 - accuracy: 0.9712\n","Epoch 31/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2351 - accuracy: 0.9686\n","Epoch 32/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2409 - accuracy: 0.9660\n","Epoch 33/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2405 - accuracy: 0.9660\n","Epoch 34/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2389 - accuracy: 0.9634\n","Epoch 35/200\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2347 - accuracy: 0.9634\n","Epoch 36/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9634\n","Epoch 37/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2353 - accuracy: 0.9660\n","Epoch 38/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2306 - accuracy: 0.9660\n","Epoch 39/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2387 - accuracy: 0.9660\n","Epoch 40/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9660\n","Epoch 41/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2300 - accuracy: 0.9712\n","Epoch 42/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2429 - accuracy: 0.9712\n","Epoch 43/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2393 - accuracy: 0.9660\n","Epoch 44/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2492 - accuracy: 0.9634\n","Epoch 45/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2515 - accuracy: 0.9686\n","Epoch 46/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2280 - accuracy: 0.9686\n","Epoch 47/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2314 - accuracy: 0.9660\n","Epoch 48/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.9660\n","Epoch 49/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9660\n","Epoch 50/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2314 - accuracy: 0.9712\n","Epoch 51/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2276 - accuracy: 0.9660\n","Epoch 52/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9660\n","Epoch 53/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9581\n","Epoch 54/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2310 - accuracy: 0.9634\n","Epoch 55/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2324 - accuracy: 0.9660\n","Epoch 56/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2261 - accuracy: 0.9634\n","Epoch 57/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2315 - accuracy: 0.9660\n","Epoch 58/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9660\n","Epoch 59/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2277 - accuracy: 0.9686\n","Epoch 60/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2222 - accuracy: 0.9660\n","Epoch 61/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2247 - accuracy: 0.9634\n","Epoch 62/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2425 - accuracy: 0.9634\n","Epoch 63/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2318 - accuracy: 0.9660\n","Epoch 64/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2331 - accuracy: 0.9660\n","Epoch 65/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9660\n","Epoch 66/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2275 - accuracy: 0.9712\n","Epoch 67/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9686\n","Epoch 68/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2170 - accuracy: 0.9686\n","Epoch 69/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2273 - accuracy: 0.9660\n","Epoch 70/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2248 - accuracy: 0.9660\n","Epoch 71/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2258 - accuracy: 0.9634\n","Epoch 72/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2278 - accuracy: 0.9660\n","Epoch 73/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.9660\n","Epoch 74/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2212 - accuracy: 0.9738\n","Epoch 75/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2267 - accuracy: 0.9660\n","Epoch 76/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2192 - accuracy: 0.9660\n","Epoch 77/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2217 - accuracy: 0.9660\n","Epoch 78/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2221 - accuracy: 0.9634\n","Epoch 79/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2209 - accuracy: 0.9660\n","Epoch 80/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2207 - accuracy: 0.9686\n","Epoch 81/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.9686\n","Epoch 82/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2161 - accuracy: 0.9634\n","Epoch 83/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2227 - accuracy: 0.9660\n","Epoch 84/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2248 - accuracy: 0.9660\n","Epoch 85/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2217 - accuracy: 0.9634\n","Epoch 86/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2228 - accuracy: 0.9660\n","Epoch 87/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2256 - accuracy: 0.9686\n","Epoch 88/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9660\n","Epoch 89/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9660\n","Epoch 90/200\n","12/12 [==============================] - 0s 8ms/step - loss: 0.2217 - accuracy: 0.9634\n","Epoch 91/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9634\n","Epoch 92/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2224 - accuracy: 0.9660\n","Epoch 93/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.9634\n","Epoch 94/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2477 - accuracy: 0.9660\n","Epoch 95/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9712\n","Epoch 96/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2218 - accuracy: 0.9529\n","Epoch 97/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2325 - accuracy: 0.9660\n","Epoch 98/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2237 - accuracy: 0.9686\n","Epoch 99/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2290 - accuracy: 0.9660\n","Epoch 100/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2208 - accuracy: 0.9712\n","Epoch 101/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2201 - accuracy: 0.9660\n","Epoch 102/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2170 - accuracy: 0.9660\n","Epoch 103/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2138 - accuracy: 0.9686\n","Epoch 104/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2213 - accuracy: 0.9660\n","Epoch 105/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2239 - accuracy: 0.9686\n","Epoch 106/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.9660\n","Epoch 107/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2135 - accuracy: 0.9686\n","Epoch 108/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2163 - accuracy: 0.9660\n","Epoch 109/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9686\n","Epoch 110/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.9660\n","Epoch 111/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2081 - accuracy: 0.9660\n","Epoch 112/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2190 - accuracy: 0.9738\n","Epoch 113/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2127 - accuracy: 0.9660\n","Epoch 114/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2092 - accuracy: 0.9686\n","Epoch 115/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2121 - accuracy: 0.9686\n","Epoch 116/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2327 - accuracy: 0.9738\n","Epoch 117/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2446 - accuracy: 0.9005\n","Epoch 118/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2530 - accuracy: 0.9712\n","Epoch 119/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.9738\n","Epoch 120/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2180 - accuracy: 0.9634\n","Epoch 121/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2285 - accuracy: 0.9660\n","Epoch 122/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2180 - accuracy: 0.9686\n","Epoch 123/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 0.9607\n","Epoch 124/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2140 - accuracy: 0.9634\n","Epoch 125/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2189 - accuracy: 0.9686\n","Epoch 126/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2151 - accuracy: 0.9660\n","Epoch 127/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2249 - accuracy: 0.9634\n","Epoch 128/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2148 - accuracy: 0.9634\n","Epoch 129/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9660\n","Epoch 130/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2170 - accuracy: 0.9555\n","Epoch 131/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2375 - accuracy: 0.9660\n","Epoch 132/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2235 - accuracy: 0.9450\n","Epoch 133/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2211 - accuracy: 0.9634\n","Epoch 134/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2145 - accuracy: 0.9660\n","Epoch 135/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2142 - accuracy: 0.9660\n","Epoch 136/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.9686\n","Epoch 137/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2119 - accuracy: 0.9660\n","Epoch 138/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2104 - accuracy: 0.9660\n","Epoch 139/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2112 - accuracy: 0.9660\n","Epoch 140/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2293 - accuracy: 0.9660\n","Epoch 141/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2181 - accuracy: 0.9581\n","Epoch 142/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9660\n","Epoch 143/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2082 - accuracy: 0.9686\n","Epoch 144/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2166 - accuracy: 0.9581\n","Epoch 145/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2044 - accuracy: 0.9686\n","Epoch 146/200\n","12/12 [==============================] - 0s 7ms/step - loss: 0.2270 - accuracy: 0.9686\n","Epoch 147/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2121 - accuracy: 0.9686\n","Epoch 148/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2167 - accuracy: 0.9607\n","Epoch 149/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2115 - accuracy: 0.9660\n","Epoch 150/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2094 - accuracy: 0.9634\n","Epoch 151/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2133 - accuracy: 0.9634\n","Epoch 152/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2411 - accuracy: 0.9607\n","Epoch 153/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2306 - accuracy: 0.9476\n","Epoch 154/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2180 - accuracy: 0.9634\n","Epoch 155/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2081 - accuracy: 0.9712\n","Epoch 156/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2130 - accuracy: 0.9686\n","Epoch 157/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2082 - accuracy: 0.9660\n","Epoch 158/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9686\n","Epoch 159/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2088 - accuracy: 0.9660\n","Epoch 160/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2083 - accuracy: 0.9660\n","Epoch 161/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9686\n","Epoch 162/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9660\n","Epoch 163/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9660\n","Epoch 164/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2199 - accuracy: 0.9372\n","Epoch 165/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9660\n","Epoch 166/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2079 - accuracy: 0.9607\n","Epoch 167/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2463 - accuracy: 0.9660\n","Epoch 168/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9607\n","Epoch 169/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2193 - accuracy: 0.9660\n","Epoch 170/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2059 - accuracy: 0.9660\n","Epoch 171/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2162 - accuracy: 0.9450\n","Epoch 172/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9660\n","Epoch 173/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9660\n","Epoch 174/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2079 - accuracy: 0.9634\n","Epoch 175/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2047 - accuracy: 0.9686\n","Epoch 176/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2071 - accuracy: 0.9686\n","Epoch 177/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9686\n","Epoch 178/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2073 - accuracy: 0.9686\n","Epoch 179/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2049 - accuracy: 0.9686\n","Epoch 180/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2065 - accuracy: 0.9686\n","Epoch 181/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2054 - accuracy: 0.9686\n","Epoch 182/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2137 - accuracy: 0.9660\n","Epoch 183/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2175 - accuracy: 0.9686\n","Epoch 184/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2109 - accuracy: 0.9529\n","Epoch 185/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2289 - accuracy: 0.9660\n","Epoch 186/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2053 - accuracy: 0.9607\n","Epoch 187/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2076 - accuracy: 0.9634\n","Epoch 188/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.9581\n","Epoch 189/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9660\n","Epoch 190/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9686\n","Epoch 191/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9686\n","Epoch 192/200\n","12/12 [==============================] - 0s 6ms/step - loss: 0.2048 - accuracy: 0.9686\n","Epoch 193/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9634\n","Epoch 194/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9660\n","Epoch 195/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.1999 - accuracy: 0.9712\n","Epoch 196/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2088 - accuracy: 0.9634\n","Epoch 197/200\n","12/12 [==============================] - 0s 4ms/step - loss: 0.2003 - accuracy: 0.9686\n","Epoch 198/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9686\n","Epoch 199/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2025 - accuracy: 0.9660\n","Epoch 200/200\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2131 - accuracy: 0.9660\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9791666666666666\n","TP Rate:  81\n","FP Rate:  1\n","Precision:  0.9878048780487805\n","Recall:  0.9878048780487805\n","F1 Score:  0.9878048780487805\n","MCC:  0.9163763066202091\n","ROC Area:  0.9581881533101045\n","PRC Area:  0.9930132113821138\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 3\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 10\n","### L5: 20\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=10, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBe42j1XfGYx","executionInfo":{"status":"ok","timestamp":1649493805682,"user_tz":-330,"elapsed":3251,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"212b0c9e-d5ce-40b5-874d-dc19509e980f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.5393\n","Epoch 2/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.6990\n","Epoch 3/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6409 - accuracy: 0.8220\n","Epoch 4/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6145 - accuracy: 0.8272\n","Epoch 5/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.5831 - accuracy: 0.8953\n","Epoch 6/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.5390 - accuracy: 0.9188\n","Epoch 7/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.9005\n","Epoch 8/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4699 - accuracy: 0.9241\n","Epoch 9/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.9241\n","Epoch 10/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.9398\n","Epoch 11/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.9267\n","Epoch 12/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.9319\n","Epoch 13/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.9346\n","Epoch 14/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.9319\n","Epoch 15/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.9319\n","Epoch 16/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.9424\n","Epoch 17/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.9450\n","Epoch 18/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.9450\n","Epoch 19/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.9450\n","Epoch 20/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.9293\n","Epoch 21/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3757 - accuracy: 0.9424\n","Epoch 22/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.9450\n","Epoch 23/24\n","32/32 [==============================] - 0s 3ms/step - loss: 0.3640 - accuracy: 0.9424\n","Epoch 24/24\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.9293\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.9375\n","TP Rate:  82\n","FP Rate:  6\n","Precision:  0.9318181818181818\n","Recall:  1.0\n","F1 Score:  0.9647058823529412\n","MCC:  0.7297037292405272\n","ROC Area:  0.7857142857142857\n","PRC Area:  0.9659090909090908\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 4\n","### Batch Size : 12\n","### Epochs: 24\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 24, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TwxtKK8fE3K","executionInfo":{"status":"ok","timestamp":1649493809739,"user_tz":-330,"elapsed":4065,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"0db9c707-ea46-4da9-f48f-b6a9b06a8657"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/24\n","32/32 [==============================] - 1s 4ms/step - loss: 0.7215 - accuracy: 0.5340\n","Epoch 2/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.3796\n","Epoch 3/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.6361\n","Epoch 4/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.7670\n","Epoch 5/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.7801\n","Epoch 6/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6027 - accuracy: 0.7853\n","Epoch 7/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5805 - accuracy: 0.7958\n","Epoch 8/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5712 - accuracy: 0.7932\n","Epoch 9/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5644 - accuracy: 0.7958\n","Epoch 10/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5615 - accuracy: 0.7958\n","Epoch 11/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5592 - accuracy: 0.7958\n","Epoch 12/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5607 - accuracy: 0.7958\n","Epoch 13/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7958\n","Epoch 14/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5481 - accuracy: 0.7958\n","Epoch 15/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7958\n","Epoch 16/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5462 - accuracy: 0.7958\n","Epoch 17/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5429 - accuracy: 0.7958\n","Epoch 18/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.7958\n","Epoch 19/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.7958\n","Epoch 20/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7958\n","Epoch 21/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5404 - accuracy: 0.7958\n","Epoch 22/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7958\n","Epoch 23/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5293 - accuracy: 0.7958\n","Epoch 24/24\n","32/32 [==============================] - 0s 4ms/step - loss: 0.5270 - accuracy: 0.7958\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.8020833333333334\n","TP Rate:  66\n","FP Rate:  3\n","Precision:  0.9565217391304348\n","Recall:  0.8048780487804879\n","F1 Score:  0.8741721854304637\n","MCC:  0.4636094954209574\n","ROC Area:  0.7952961672473868\n","PRC Area:  0.9640332272887947\n","-----------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Deep Learning Model - Top 50 Config 5\n","### Batch Size : 12\n","### Epochs: 200\n","### L1: 2    \n","### L2: 20\n","### L3: 200\n","### L4: 400\n","### L5: 2\n","\n","ann = tf.keras.models.Sequential()\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=20, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=200, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=400, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=2, activation='relu'))\n","ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","ann.fit(X_train, y_train, batch_size = 12, epochs = 200, class_weight=class_weights)\n","\n","y_pred = ann.predict(X_test)\n","y_pred = (y_pred > 0.5)\n","\n","CM = confusion_matrix(y_test, y_pred)\n","TP = CM[1][1]\n","FP = CM[0][1]\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n","prc = auc(recall, precision)\n","\n","for _ in range(3):\n","  print()\n","print(\"-----------------------------------------------------------------------------\")\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(\"TP Rate: \", TP)\n","print(\"FP Rate: \", FP)\n","print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n","print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n","print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n","print(\"MCC: \", metrics.matthews_corrcoef(y_test, y_pred))\n","print(\"ROC Area: \", metrics.roc_auc_score(y_test, y_pred))\n","print(\"PRC Area: \", prc)\n","print(\"-----------------------------------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTTJkCGNew5j","executionInfo":{"status":"ok","timestamp":1649493838290,"user_tz":-330,"elapsed":28561,"user":{"displayName":"Akshat Rastogi","userId":"00694653395562317316"}},"outputId":"b4153341-652a-4466-f3cb-793e2426e3ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","32/32 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.8403\n","Epoch 2/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6952 - accuracy: 0.8455\n","Epoch 3/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6942 - accuracy: 0.5576\n","Epoch 4/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.1649\n","Epoch 5/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 6/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3482\n","Epoch 7/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1545\n","Epoch 8/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2016\n","Epoch 9/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1545\n","Epoch 10/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 11/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 12/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3010\n","Epoch 13/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1702\n","Epoch 14/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 15/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2853\n","Epoch 16/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3115\n","Epoch 17/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 18/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 19/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 20/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 21/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4319\n","Epoch 22/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5890\n","Epoch 23/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1963\n","Epoch 24/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 25/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3743\n","Epoch 26/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 27/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 28/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 29/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 30/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 31/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1545\n","Epoch 32/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3377\n","Epoch 33/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.1545\n","Epoch 34/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 35/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 36/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2487\n","Epoch 37/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 38/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 39/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 40/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 41/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4581\n","Epoch 42/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 43/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 44/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6623\n","Epoch 45/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 46/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2906\n","Epoch 47/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4791\n","Epoch 48/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5314\n","Epoch 49/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.6047\n","Epoch 50/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7932\n","Epoch 51/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2382\n","Epoch 52/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.6728\n","Epoch 53/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2906\n","Epoch 54/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2225\n","Epoch 55/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 56/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1963\n","Epoch 57/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4005\n","Epoch 58/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2958\n","Epoch 59/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 60/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 61/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2958\n","Epoch 62/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 63/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4529\n","Epoch 64/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4267\n","Epoch 65/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 66/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 67/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7880\n","Epoch 68/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7984\n","Epoch 69/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.3691\n","Epoch 70/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6047\n","Epoch 71/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 72/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.8455\n","Epoch 73/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 74/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5733\n","Epoch 75/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7775\n","Epoch 76/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2225\n","Epoch 77/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2330\n","Epoch 78/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7984\n","Epoch 79/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6885\n","Epoch 80/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 81/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 82/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2644\n","Epoch 83/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 84/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 85/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7775\n","Epoch 86/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.3010\n","Epoch 87/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7356\n","Epoch 88/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 89/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3272\n","Epoch 90/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5628\n","Epoch 91/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.7513\n","Epoch 92/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2696\n","Epoch 93/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2016\n","Epoch 94/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8141\n","Epoch 95/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 96/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 97/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3796\n","Epoch 98/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.3743\n","Epoch 99/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.3168\n","Epoch 100/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6518\n","Epoch 101/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3691\n","Epoch 102/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7932\n","Epoch 103/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 104/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 105/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.8141\n","Epoch 106/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2539\n","Epoch 107/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 108/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6832\n","Epoch 109/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6466\n","Epoch 110/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 111/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 112/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2539\n","Epoch 113/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2749\n","Epoch 114/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5419\n","Epoch 115/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2906\n","Epoch 116/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2801\n","Epoch 117/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 118/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 119/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.8455\n","Epoch 120/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 121/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4581\n","Epoch 122/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.4634\n","Epoch 123/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8141\n","Epoch 124/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 125/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4581\n","Epoch 126/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7723\n","Epoch 127/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5942\n","Epoch 128/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4162\n","Epoch 129/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 130/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1649\n","Epoch 131/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7984\n","Epoch 132/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 133/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 134/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3325\n","Epoch 135/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2696\n","Epoch 136/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 137/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 138/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 139/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4215\n","Epoch 140/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7094\n","Epoch 141/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 142/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 143/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3272\n","Epoch 144/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 145/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3901\n","Epoch 146/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3010\n","Epoch 147/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5942\n","Epoch 148/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.2644\n","Epoch 149/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7880\n","Epoch 150/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.4581\n","Epoch 151/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.2906\n","Epoch 152/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6152\n","Epoch 153/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 154/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 155/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 156/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 157/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6361\n","Epoch 158/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2330\n","Epoch 159/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1806\n","Epoch 160/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5052\n","Epoch 161/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 162/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8141\n","Epoch 163/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4110\n","Epoch 164/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4948\n","Epoch 165/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4110\n","Epoch 166/200\n","32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 167/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 168/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7042\n","Epoch 169/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4529\n","Epoch 170/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.1545\n","Epoch 171/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.6099\n","Epoch 172/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5524\n","Epoch 173/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2487\n","Epoch 174/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7408\n","Epoch 175/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 176/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 177/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 178/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4738\n","Epoch 179/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6257\n","Epoch 180/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.3063\n","Epoch 181/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.8141\n","Epoch 182/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 183/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 184/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.8455\n","Epoch 185/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 186/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.3691\n","Epoch 187/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 188/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 189/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4686\n","Epoch 190/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.7304\n","Epoch 191/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 192/200\n","32/32 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.8455\n","Epoch 193/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4686\n","Epoch 194/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.1545\n","Epoch 195/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.7408\n","Epoch 196/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.8455\n","Epoch 197/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2330\n","Epoch 198/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.7618\n","Epoch 199/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.2801\n","Epoch 200/200\n","32/32 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.1545\n","\n","\n","\n","-----------------------------------------------------------------------------\n","Accuracy:  0.14583333333333334\n","TP Rate:  0\n","FP Rate:  0\n","Precision:  0.0\n","Recall:  0.0\n","F1 Score:  0.0\n","MCC:  0.0\n","ROC Area:  0.5\n","PRC Area:  0.9270833333333333\n","-----------------------------------------------------------------------------\n"]}]}]}